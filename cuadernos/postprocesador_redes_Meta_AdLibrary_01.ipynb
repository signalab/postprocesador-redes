{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDZCDjTAn5JW"
      },
      "source": [
        "# **Signa_Lab ITESO:** Postprocesador de datos de redes sociodigitales\n",
        "## **Cuaderno: Meta AdLibrary (listas de anuncios)**\n",
        "\n",
        "Cuaderno de código abierto diseñado para importar datos sobre listas de las plataforma de Meta (Facebook, Instagram, Messenger, Audience Network) descargados de la plataforma Meta AdLibrary, y preparar sus datos para su posterior análisis exploratorio. Este cuaderno aporta también un análisis desagregado de las estrategias de segmentación y alcance aplicadas para cada anuncio y transparentadas en Meta*.\n",
        "\n",
        "**\\***Los grupos de celdas marcadas con **asterisco requieren información** antes de seguir adelante, que podrás agregas en los campos a llenar que desplieguen por esas celdas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU6rrB09h8IO"
      },
      "source": [
        "## **0. Introducción**\n",
        "\n",
        "Antes de comenzar, necesitas obtener al menos un archivo en formato **CSV o Excel** que registre **listas de anuncios** y sus metadatos, descargables de Meta AdLibrary.\n",
        "\n",
        "Puedes leer la documentación de la [plataforma web](https://www.facebook.com/ads/library/) y de [la API](https://www.facebook.com/ads/library/api/) para saber más sobre los datos consultables y cómo interpretar los campos que arrojan.\n",
        "\n",
        "La [plataforma web MetaAdLibrary](https://www.facebook.com/ads/library/) permite descargar datos de anuncios como archivo CSV.\n",
        "\n",
        "Se pueden hacer **hasta 3 descargas por día**, exclusivamente sobre anuncios marcados por Meta como ***Temas sociales, elecciones o política***.\n",
        "\n",
        "Permite **formular búsquedas**, opcionalmente acotadasa a algún país, por:\n",
        "* Páginas de anunciantes específicos\n",
        "* Anuncios relevantes por palabras clave o frases exactas\n",
        "* Anuncios segmentados a cierta ubicación (*Región* o *Ciudad*)\n",
        "\n",
        "A su vez se pueden **filtrar los resultados** de tu búsqueda por parámetros como:\n",
        "* Idioma\n",
        "* Anunciante (lista completa de anunciantes en los resultados de la búsqueda)\n",
        "* Plataforma (*Facebook*, *Instagram*, *Audience Network*, *Messenger*)\n",
        "* Tipo de contenido multimedia (*Imágenes*, *Memes*, *Memes y videos*, *Videos*, *Sin imagen ni video*)\n",
        "* Estado (*Activos e inactivos*, *Anuncios activos*, *Anuncios inactivos*)\n",
        "* Impresiones por fecha (intervalo por fechas exactas)\n",
        "\n",
        "Para poder descargar los archivos en CSV, es necesario haber iniciado sesión, aunque la plataforma se pueden explorar resultados de búsqueda sin haberlo hecho.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agO7lzQExahT"
      },
      "source": [
        "## **1. Importar librerías y archivos de datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krmVOSpgxeyN"
      },
      "source": [
        "### Instalar e importar librerías:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SvIo0aTWncju"
      },
      "outputs": [],
      "source": [
        "# Instalar librerías de Python necesarias\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install simpledbf\n",
        "!pip install nltk\n",
        "!pip install folium pandas\n",
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSqV2J7obSXG"
      },
      "outputs": [],
      "source": [
        "# Importar librerías de Python necesarias\n",
        "import pandas as pd\n",
        "import copy as copy\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import simpledbf\n",
        "\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import plotly.express as px\n",
        "import geopandas as gpd\n",
        "from simpledbf import Dbf5\n",
        "import seaborn as sns\n",
        "\n",
        "import unidecode\n",
        "import folium\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlViHlO_x5lv"
      },
      "source": [
        "### *Indicar rutas de archivos de datos a importar y nombre de proyecto:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku0cNearyAAb"
      },
      "source": [
        "**Importar y concatenar archivos de datos** (en CSV o Excel):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNE1_aEJyDHo"
      },
      "outputs": [],
      "source": [
        "# Definir función para cargar archivos a partir de la extensión en su ruta indicada\n",
        "def load_file(path):\n",
        "    if path.endswith('.csv'):\n",
        "        return pd.read_csv(path)\n",
        "    elif path.endswith('.xlsx'):\n",
        "        return pd.read_excel(path)\n",
        "    else:\n",
        "        raise ValueError(\"Formato no compatible. Por favor carga solo archivos .csv or .xlsx.\")\n",
        "\n",
        "# Inicializar lista para alojar todas las rutas y una variable para el DataFrame final, accesible globalmente\n",
        "file_paths = []\n",
        "dfs = []\n",
        "df = None  # DataFrame global\n",
        "\n",
        "# Definir función para añadir un nuevo campo de texto (input) para añadir una ruta de archivo adicional\n",
        "def add_file_input(b=None):\n",
        "    path_input = widgets.Text(value='', placeholder='Escribe la ruta del archivo', description=f'Archivo {len(file_paths) + 1}:')\n",
        "    file_paths.append(path_input)\n",
        "    update_ui()\n",
        "\n",
        "# Definir función para eliminar el último campo de texto (input) para ruta de archivo\n",
        "def remove_file_input(b=None):\n",
        "    if file_paths:\n",
        "        file_paths.pop()\n",
        "        update_ui()\n",
        "\n",
        "# Definir función para procesar y cargar todos los archivos\n",
        "def process_files(b):\n",
        "    global dfs, df\n",
        "    dfs = []  # Vaciar DataFrames\n",
        "\n",
        "    for path_input in file_paths:\n",
        "        path = path_input.value\n",
        "        try:\n",
        "            temp_df = load_file(path)\n",
        "            temp_df['filename'] = path  # Add a column with the filename\n",
        "            dfs.append(temp_df)\n",
        "            print(f\"Nombre de archivo: {path}\")\n",
        "            print(f\"Filas/Columnas (shape): {temp_df.shape}\")\n",
        "        except ValueError as e:\n",
        "            print(f\"Error al cargar el archivo {path}: {e}\")\n",
        "            return\n",
        "\n",
        "    if dfs:\n",
        "        df = pd.concat(dfs, ignore_index=True)  # Concatenate all DataFrames\n",
        "        print(\"\\n¡Se cargaron todos los archivos!\")\n",
        "        print(f\"Filas/Columnas (shape) de DataFrame creado: {df.shape}\")\n",
        "\n",
        "# Campo de texto (input) para indicar nombre del proyecto (para integrarse en nombres de archivos a exportar)\n",
        "project_name = widgets.Text(value='', placeholder='Escribe el nombre del proyecto (corto y sin espacios)', description='Nombre proyecto:')\n",
        "\n",
        "# Botones para añadir y eliminar archivos\n",
        "add_button = widgets.Button(description=\"Añadir archivo\",  button_style='')\n",
        "remove_button = widgets.Button(description=\"Eliminar archivo\",  button_style='warning')\n",
        "load_button = widgets.Button(description=\"Cargar archivos\",  button_style='primary')\n",
        "\n",
        "add_button.on_click(add_file_input)\n",
        "remove_button.on_click(remove_file_input)\n",
        "load_button.on_click(process_files)\n",
        "\n",
        "# Definir función para actualizar UI\n",
        "def update_ui():\n",
        "    clear_output()\n",
        "    display(project_name)\n",
        "    for path_input in file_paths:\n",
        "        display(path_input)\n",
        "    display(widgets.HBox([add_button, remove_button]))\n",
        "    display(load_button)\n",
        "\n",
        "# Inicializar UI con un campo de texto (input) para ruta de archivo\n",
        "add_file_input()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VtBL2ysyAkF"
      },
      "source": [
        "### Previsualizar datos importados:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUGdxfP7yJuS"
      },
      "source": [
        "**Previsualizar tabla** con todos los registros importados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeSbQGhtz51L"
      },
      "outputs": [],
      "source": [
        "## Previsualizar dataframe con CSVs importados\n",
        "display(df)\n",
        "print(f\"Filas/Columnas (shape) en registros importados: {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KIb_xnXyQxk"
      },
      "source": [
        "**Exportar copia en CSV con registros importados (o concatenados):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJbMLJUwySj4"
      },
      "outputs": [],
      "source": [
        "# Exportar archivo CSV con tabla de registros importados (y concatenados, en el caso de múltiples archivos)\n",
        "df.to_csv(f\"{project_name.value}_registros-importados.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6b_rWUJyW6M"
      },
      "source": [
        "## **2. Limpieza y procesamient de registros importados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfE26SQw6oF3"
      },
      "source": [
        "### Generar identificadores únicos (IDs) por registro:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9IXQz_I6nRy"
      },
      "outputs": [],
      "source": [
        "# Definir función para asignar IDs únicos a cada fila en el data frame indicado como parámetro, comenzando desde '1000001'.\n",
        "def assign_unique_ids(df):\n",
        "    # Inicializar contador para IDs\n",
        "    id_counter = 1000001\n",
        "\n",
        "    # Crear copia de data frame original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Iterar a través de las filas del data frame\n",
        "    for index, _ in enumerate(df_copy.index):\n",
        "        # Dar formato a ID con ceros adicionales e incorporarlo al data frame\n",
        "        formatted_id = str(id_counter).zfill(7)  # Se asegura de que sea un ID de 7 dígitos, agregando ceros cuando sea necesario\n",
        "        df_copy.loc[index, 'id'] = formatted_id\n",
        "\n",
        "        # Incrementar el contador del ID para la siguiente iteración\n",
        "        id_counter += 1\n",
        "\n",
        "    return df_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8ayU-vw6t7i"
      },
      "outputs": [],
      "source": [
        "# Ejecutar función para asignar IDs a cada registro y previsualizar tabla\n",
        "if __name__ == \"__main__\":\n",
        "    # Invocar la función con data frame de trabajo\n",
        "    df_ids = assign_unique_ids(df)\n",
        "\n",
        "# Sobreescribir data frame con nueva tabla con IDs generados\n",
        "df = df_ids\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYzRA7yY7Ote"
      },
      "source": [
        "### *Elegir columna y criterios para limpieza de texto sin aporte semántico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzABkmElJYxf"
      },
      "outputs": [],
      "source": [
        "# Definir función para limpiar usuarios, hashtags y URLs\n",
        "def limpiar_texto(texto, eliminar_usuarios, eliminar_hashtags, eliminar_urls, regex_personalizado):\n",
        "    # Eliminar usuarios si está activado\n",
        "    if eliminar_usuarios:\n",
        "        texto = re.sub(r\"(?<!\\w)@(\\w+)(?!\\w)\", \"\", texto)\n",
        "\n",
        "    # Eliminar hashtags si está activado\n",
        "    if eliminar_hashtags:\n",
        "        texto = re.sub(r\"(?<!\\w)#(\\w+)(?!\\w)\", \"\", texto)\n",
        "\n",
        "    # Eliminar URLs si está activado\n",
        "    if eliminar_urls:\n",
        "        texto = re.sub(r\"(http|https|ftp)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\", \"\", texto)\n",
        "        texto = texto.lstrip(\". \")\n",
        "\n",
        "    # Aplicar regex personalizado si se proporciona\n",
        "    if regex_personalizado:\n",
        "        texto = re.sub(regex_personalizado, \"\", texto)\n",
        "\n",
        "    return texto.strip()\n",
        "\n",
        "# Definir función para agregar nueva columna con texto limpio (clean_text)\n",
        "def agregarCleanTextADf(df, colText, eliminar_usuarios, eliminar_hashtags, eliminar_urls, regex_personalizado):\n",
        "    dfW = df.copy()\n",
        "    dfW[\"clean_text\"] = None\n",
        "\n",
        "    for index, row in dfW.iterrows():\n",
        "        text = str(row[colText])\n",
        "        cleaned_text = limpiar_texto(text, eliminar_usuarios, eliminar_hashtags, eliminar_urls, regex_personalizado)\n",
        "        dfW.at[index, \"clean_text\"] = cleaned_text\n",
        "\n",
        "    return dfW\n",
        "\n",
        "\n",
        "# Crear widget de desplegable con columnas disponibles\n",
        "available_columns = list(df.columns)\n",
        "column_dropdown = widgets.Dropdown(options=available_columns, value=available_columns[0], description='Columna:')\n",
        "\n",
        "\n",
        "# Definir widgets de IPyWidgets para la UI interactiva\n",
        "eliminar_usuarios = widgets.Checkbox(value=True, description='Eliminar usuarios @')\n",
        "eliminar_hashtags = widgets.Checkbox(value=True, description='Eliminar hashtags #')\n",
        "eliminar_urls = widgets.Checkbox(value=True, description='Eliminar URLs')\n",
        "regex_input = widgets.Text(value='', description='Otro (Regex):', placeholder='Escribe un regex opcional')\n",
        "\n",
        "\n",
        "# Botón para ejecutar la limpieza\n",
        "boton_limpiar = widgets.Button(description=\"Limpiar texto\", button_style='warning')\n",
        "\n",
        "# Caja de texto para mostrar el resultado\n",
        "output_resultado = widgets.Output()\n",
        "\n",
        "# Función que se ejecuta al hacer clic en el botón de limpiar texto\n",
        "def ejecutar_limpieza(b):\n",
        "    global dfCleanText  # Hacer que dfCleanText sea accesible globalmente\n",
        "    with output_resultado:\n",
        "        output_resultado.clear_output()  # Limpiar cualquier salida previa\n",
        "\n",
        "        # Obtener el nombre de la columna seleccionada por el usuario\n",
        "        text_column = column_dropdown.value\n",
        "\n",
        "        # Verificar si la columna existe en el DataFrame\n",
        "        if text_column not in df.columns:\n",
        "            print(f\"Error: La columna '{text_column}' no existe en el DataFrame.\")\n",
        "            return\n",
        "\n",
        "        # Ejecutar la función de limpieza sobre el DataFrame seleccionado\n",
        "        dfCleanText = agregarCleanTextADf(\n",
        "            df, text_column,\n",
        "            eliminar_usuarios.value,\n",
        "            eliminar_hashtags.value,\n",
        "            eliminar_urls.value,\n",
        "            regex_input.value\n",
        "        )\n",
        "\n",
        "        # Mostrar el DataFrame con la nueva columna 'clean_text'\n",
        "        print(\"Texto limpio aplicado. DataFrame actualizado globalmente como 'dfCleanText'.\")\n",
        "        display(dfCleanText)\n",
        "\n",
        "# Conectar el botón con la función de limpieza\n",
        "boton_limpiar.on_click(ejecutar_limpieza)\n",
        "\n",
        "# Desplegar los widgets en pantalla\n",
        "display(column_dropdown, eliminar_usuarios, eliminar_hashtags, eliminar_urls, regex_input, boton_limpiar, output_resultado)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXl-EjuU7XMr"
      },
      "source": [
        "### \\*Elegir idioma de palabras vacías (*stop words*) a eliminar:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_nWCHsy7X_f"
      },
      "source": [
        "Elegir **idioma de** (desde diccionarios de NLTK), **eliminar *stop words*** y **agregar columna sem_text** con texto depurado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3qUIVd27T_P"
      },
      "outputs": [],
      "source": [
        "# Definir función para eliminar stopwords y signos de puntuación\n",
        "def delete_stopwords(texto, stopwords_list):\n",
        "    # Tokenizar el texto\n",
        "    tokens = nltk.word_tokenize(texto)\n",
        "\n",
        "    # Eliminar signos de puntuación\n",
        "    tokens = [token for token in tokens if token.isalnum()]\n",
        "\n",
        "    # Eliminar stop words\n",
        "    tokens = [token for token in tokens if token.lower() not in stopwords_list]\n",
        "\n",
        "    # Convertir la lista de tokens a un string\n",
        "    texto_limpio = \" \".join(tokens)\n",
        "\n",
        "    return texto_limpio.strip()\n",
        "\n",
        "# Definir función para eliminar palabras vacías y agregar nueva columna con el resultado (sem_text)\n",
        "def agregarSemTextADf(df, colText, stopwords_list):\n",
        "    dfW = df.copy()\n",
        "    dfW[\"sem_text\"] = None\n",
        "\n",
        "    for index, row in dfW.iterrows():\n",
        "        text = row[colText]\n",
        "        cleaned_text = delete_stopwords(text, stopwords_list)\n",
        "        dfW.at[index, \"sem_text\"] = cleaned_text\n",
        "\n",
        "    return dfW\n",
        "\n",
        "# Lista de idiomas disponibles en NLTK\n",
        "idiomas_stopwords = [\n",
        "    'arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french',\n",
        "    'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali',\n",
        "    'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish',\n",
        "    'swedish', 'tajik', 'turkish'\n",
        "]\n",
        "\n",
        "# Crear dropdown para seleccionar idioma\n",
        "dropdown_idioma = widgets.Dropdown(\n",
        "    options=idiomas_stopwords,\n",
        "    value='spanish',  # Valor por defecto\n",
        "    description='Idioma:'\n",
        ")\n",
        "\n",
        "# Botón para eliminar stopwords\n",
        "boton_eliminar_stopwords = widgets.Button(description=\"Eliminar stopwords\", button_style='warning')\n",
        "\n",
        "# Output para mostrar el resultado\n",
        "output_resultado_stopwords = widgets.Output()\n",
        "\n",
        "# Función que se ejecuta al hacer clic en el botón de eliminar stopwords\n",
        "def ejecutar_eliminacion_stopwords(b):\n",
        "    global dfClean  # Hacer que dfClean sea accesible globalmente\n",
        "    with output_resultado_stopwords:\n",
        "        output_resultado_stopwords.clear_output()  # Limpiar cualquier salida previa\n",
        "\n",
        "        # Obtener el idioma seleccionado\n",
        "        idioma_seleccionado = dropdown_idioma.value\n",
        "\n",
        "        # Descargar stopwords del idioma seleccionado\n",
        "        stopwords_list = stopwords.words(idioma_seleccionado)\n",
        "\n",
        "        # Verificar si la columna 'clean_text' existe en el DataFrame\n",
        "        if 'clean_text' not in dfCleanText.columns:\n",
        "            print(\"Error: La columna 'clean_text' no existe en el DataFrame.\")\n",
        "            return\n",
        "\n",
        "        # Ejecutar la función para eliminar stopwords y agregar la nueva columna 'sem_text'\n",
        "        dfClean = agregarSemTextADf(dfCleanText, 'clean_text', stopwords_list)\n",
        "\n",
        "        # Mostrar el DataFrame con la nueva columna 'sem_text'\n",
        "        print(f\"Stopwords en {idioma_seleccionado} eliminadas. DataFrame actualizado globalmente como 'dfClean'.\")\n",
        "        display(dfClean)\n",
        "\n",
        "# Conectar el botón con la función de eliminación de stopwords\n",
        "boton_eliminar_stopwords.on_click(ejecutar_eliminacion_stopwords)\n",
        "\n",
        "# Desplegar los widgets en pantalla\n",
        "display(dropdown_idioma, boton_eliminar_stopwords, output_resultado_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyVU2PR91HVP"
      },
      "source": [
        "### Calcular valores promedio de audiencia, impresiones y costo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIBdJXtB2gpV"
      },
      "source": [
        "Definir función para **extraer los valores numéricos** de rangos dados por Meta en campos de ***audience*, *impressions* y *spend***, y **promediarlos** (en caso de registrar un límite inferior y superior)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOpff1rCNblp"
      },
      "outputs": [],
      "source": [
        "# Sobreescribir df general con datos limpios\n",
        "df = dfClean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVYDlJxQ0tWx"
      },
      "outputs": [],
      "source": [
        "# Función para convertir una cadena de entrada en un diccionario\n",
        "def string_to_dict(input_string):\n",
        "    # Definir regex para encontrar diferentes formatos de límites en strings con rang\n",
        "    patterns = [\n",
        "        r'lower_bound: (\\d+), upper_bound: (\\d+)',\n",
        "        r'lower_bound: (\\d+)',\n",
        "        r'upper_bound: (\\d+)'\n",
        "    ]\n",
        "\n",
        "    # Inicializar un diccionario de resultados con valores por defecto de None para ambos límites\n",
        "    result_dict = {'lower_bound': None, 'upper_bound': None}\n",
        "\n",
        "    # Iterar sobre los patrones de Regex definidos:\n",
        "    for pattern in patterns:\n",
        "        # Buscar una coincidencia del patrón en string\n",
        "        match = re.search(pattern, input_string)\n",
        "        if match:\n",
        "            # Si se encuentra una coincidencia, extraer el valor correspondiente del límite y almacenarlo en el diccionario de resultados\n",
        "            if 'lower_bound' in pattern:\n",
        "                result_dict['lower_bound'] = int(match.group(1))\n",
        "            if 'upper_bound' in pattern:\n",
        "                result_dict['upper_bound'] = int(match.group(1))\n",
        "            break\n",
        "\n",
        "    # Devolver el diccionario con los valores extraídos\n",
        "    return result_dict\n",
        "\n",
        "# Función para extraer diccionarios de string en bruto\n",
        "def extract_dicts(raw_str):\n",
        "    # Definir un patrón para encerrar cualquier contenido entre llaves\n",
        "    pattern = r'\\{.*?\\}'\n",
        "    # Buscar todas las coincidencias del patrón en la representación de cadena del DataFrame\n",
        "    matches = re.findall(pattern, str(raw_str))\n",
        "    # Convertir cada coincidencia extraída en un diccionario Python usando eval()\n",
        "    dicts = [eval(match) for match in matches]\n",
        "\n",
        "    # Devolver una lista de diccionarios extraídos\n",
        "    return dicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7tTkFFX3PHx"
      },
      "source": [
        "Ejecutar procesamiento de datos para procesar formatos de fechas y **obtener los siguientes nuevos campos**:\n",
        "\n",
        "**Rangos y promedio de dinero** invertido en anuncio:\n",
        "* spend_min\n",
        "* spend_max\n",
        "* spend_avg\n",
        "\n",
        "**Rangos y promedio de impresiones** efectivas:\n",
        "* impressions_min\n",
        "* impressions_max\n",
        "* impressions_avg\n",
        "\n",
        "**Rangos y promedio de audiencia** potencial segmentada:\n",
        "* audience_min\n",
        "* audience_max\n",
        "* audience_avg\n",
        "\n",
        "**Duración** de campaña (en días):\n",
        "* campaign_duration\n",
        "\n",
        "**URLs exactas** de página y anuncio:\n",
        "* page_url\n",
        "* ad_url\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQnbjvwa-ppH"
      },
      "outputs": [],
      "source": [
        "#Formato datetime\n",
        "df['ad_creation_time'] = pd.to_datetime(df['ad_creation_time'])\n",
        "df['ad_delivery_start_time'] = pd.to_datetime(df['ad_delivery_start_time'])\n",
        "df['ad_delivery_stop_time'] = pd.to_datetime(df['ad_delivery_stop_time'])\n",
        "\n",
        "#Transformar en diccionarios los datos de las columnas\n",
        "df['spend_dict'] = df['spend'].apply(string_to_dict)\n",
        "df['impressions_dict'] = df['impressions'].apply(string_to_dict)\n",
        "df['estimated_audience_size_dict'] = df['estimated_audience_size'].apply(string_to_dict)\n",
        "\n",
        "#Crear nuevas columnas\n",
        "df['spend_min'] = df['spend_dict'].apply(lambda x: x['lower_bound'])\n",
        "df['spend_max'] = df['spend_dict'].apply(lambda x: x['upper_bound'])\n",
        "df['impressions_min'] = df['impressions_dict'].apply(lambda x: x['lower_bound'])\n",
        "df['impressions_max'] = df['impressions_dict'].apply(lambda x: x['upper_bound'])\n",
        "df['spend_avg'] = (df['spend_min'] + df['spend_max']) / 2\n",
        "df['impressions_avg'] = (df['impressions_min'] + df['impressions_max']) / 2\n",
        "\n",
        "# Extract audience_min and audience_max\n",
        "df['audience_min'] = df['estimated_audience_size_dict'].apply(lambda x: x['lower_bound'])\n",
        "df['audience_max'] = df['estimated_audience_size_dict'].apply(lambda x: x['upper_bound'])\n",
        "\n",
        "# Calculate audience_avg\n",
        "df['audience_avg'] = df.apply(lambda row: (row['audience_min'] + row['audience_max']) / 2 if pd.notna(row['audience_max']) else row['audience_min'], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "df['page_url'] = \"https://www.facebook.com/ads/library/?id=\" + df['page_id'].astype(str)\n",
        "df['ad_url'] = \"https://www.facebook.com/\" + df['ad_archive_id'].astype(str)\n",
        "\n",
        "df['campaign_duration'] = df['ad_delivery_stop_time'] - df['ad_delivery_start_time']\n",
        "df['campaign_duration'] = df['campaign_duration'].dt.days\n",
        "\n",
        "# Extraer diccionarios\n",
        "df['demographic_distribution_dict'] = df['demographic_distribution'].apply(extract_dicts)\n",
        "df['delivery_by_region_dict'] = df['delivery_by_region'].apply(extract_dicts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNj-SmNzCm6X"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVFZU8O2UQYC"
      },
      "outputs": [],
      "source": [
        "# Exportar archivo CSV con datos limpios y procesados\n",
        "df.to_csv(f\"{project_name.value}_procesados.csv\")\n",
        "\n",
        "print(f\"{project_name.value}_procesados.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfXDMD8T8oSy"
      },
      "source": [
        "## **3. Depuración de registros desde diccionarios personalizados (con términos de filtrado o descarte)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFGvqSeK8r3c"
      },
      "source": [
        "### *Definir uso de de diccionario de depuración:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m18jtOz8vLq"
      },
      "source": [
        "A partir de diccionarios personalizados con términos de descarte o filtración, elige depurar o mantener, respectivamente, registros que los contengan.\n",
        "\n",
        "El diccionario debe cargarse en formato CSV y contener, al menos, los siguientes campos:\n",
        "\n",
        "| palabra | tipo | categoría | diccionario |\n",
        "|---------|------|-----------|-------------|\n",
        "|         |      |           |             |\n",
        "|         |      |           |             |\n",
        "|         |      |           |             |\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "| palabra | tipo   | categoría   | diccionario        |\n",
        "|---------|--------|-------------|--------------------|\n",
        "| idiota  | ofensa | humillación | ofensa-humillación |\n",
        "| zorra   | ofensa | género      | ofensa-género      |\n",
        "|         |        |             |                    |\n",
        "\n",
        "Puedes [encontrar aquí una copia del diccionario](https://drive.google.com/file/d/1zK214W0pBRYn9lnY_MDJYhEEc6pI3L6F/view?usp=sharing) con la estructura requerida, en formato CSV, para descargar, llenar e incorporar a este cuaderno de código.\n",
        "\n",
        "**Nota:** La versión actual de este cuaderno de código permite elegir entre las siguientes opciones sobre la carga de diccionarios:\n",
        "\n",
        "\n",
        "- ***No cargar diccionario:*** Ignorar esta funcionalidad y no usar diccionarios.\n",
        "- ***Diccionario de descarte (negativo):*** Se eliminarán todos los registros que mencionen alguno de esos términos.\n",
        "- ***Diccionario de filtrado (positivo):*** Se mantendrán solo los registros que mencionen alguno de esos términos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd7Z8GhI8zgg"
      },
      "outputs": [],
      "source": [
        "# Create the dropdown widget for dictionary usage selection\n",
        "cargar_diccionario = widgets.Dropdown(\n",
        "    options=[\"No cargar diccionario\", \"Diccionario de descarte (negativo)\", \"Diccionario de filtrado (positivo)\"],\n",
        "    value=\"No cargar diccionario\",\n",
        "    description=\"Elegir uso:\"\n",
        ")\n",
        "\n",
        "# Create the text input widget for file path\n",
        "rutaDicc = widgets.Text(\n",
        "    value=\"\",\n",
        "    placeholder=\"Escribir ruta a archivo CSV\",\n",
        "    description=\"Ruta:\"\n",
        ")\n",
        "\n",
        "# Create the \"Aceptar\" button\n",
        "accept_button = widgets.Button(description=\"Aceptar\")\n",
        "\n",
        "# Create an output widget to display results or errors\n",
        "output = widgets.Output()\n",
        "\n",
        "# Function to load the dictionary based on user input\n",
        "def load_dictionary(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "            try:\n",
        "                global dfDiccionario\n",
        "                dfDiccionario = pd.read_csv(rutaDicc.value)\n",
        "                print(f\"Archivo de diccionario '{rutaDicc.value}' cargado exitosamente.\")\n",
        "                display(dfDiccionario.head())  # Show the first few rows of the loaded dictionary\n",
        "            except FileNotFoundError:\n",
        "                print(f\"El archivo de diccionario '{rutaDicc.value}' no fue encontrado. Por favor, verifique la ruta.\")\n",
        "        else:\n",
        "            dfDiccionario = None\n",
        "            print(\"No se cargó ningún diccionario.\")\n",
        "\n",
        "# Link the button to the load function\n",
        "accept_button.on_click(load_dictionary)\n",
        "\n",
        "# Display the widgets\n",
        "display(cargar_diccionario, rutaDicc, accept_button, output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nf6b7Afr8_3I"
      },
      "outputs": [],
      "source": [
        "# Verificar uso de diccionario elegido\n",
        "print(f\"Uso de diccionario elegido: {cargar_diccionario.value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOfAWch29EUR"
      },
      "source": [
        "### Aplicar filtrado por diccionario y previsualizar resultados (opcional, solo si se cargó algún diccionario):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU_Ktfvk9GRi"
      },
      "source": [
        "**Previsualizar diccionario de descarte importado:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8SwN9bf9JPG"
      },
      "outputs": [],
      "source": [
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  # Previsualizar tabla de diccionario cargado (en caso de haber elegido utilizar diccionarios)\n",
        "  try:\n",
        "      display(dfDiccionario.head())  # Previsualizar primeros registros de diccionario\n",
        "      print(f\"\\nFilas y columnas en diccionario cargado:\")  # Verificar número de filas y columnas en diccionario\n",
        "      print(f\"Shape: {dfDiccionario.shape}\")  # Verificar número de filas y columnas en diccionario\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoyKeokx9M1U"
      },
      "source": [
        "**Filtrar registros a partir de diccionario cargado y modalidad de depuración (positiva o negativa):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Uw5jQsJ9PI-"
      },
      "outputs": [],
      "source": [
        "def filtrar_registros(df_registros, df_terminos, colTexto, cargar_diccionario):\n",
        "    # Definir los términos del diccionario\n",
        "    terminos = df_terminos[\"palabra\"].tolist()\n",
        "    # Compilar expresiones regulares una sola vez\n",
        "    expresiones_regex = [\n",
        "        re.compile(r\"(?<!\\S)?(?:\\s|[.,;:?!¡¿]){}(?:\\s|[.,;:?!¡¿])?(?!\\S)\".format(re.escape(termino)), re.IGNORECASE)\n",
        "        for termino in terminos\n",
        "    ]\n",
        "\n",
        "    # Copiar el dataframe de registros y agregar columnas auxiliares\n",
        "    df_registros_filtrados = df_registros.copy()\n",
        "    df_registros_filtrados[\"contiene_termino\"] = False\n",
        "    df_registros_filtrados[\"razon_eliminacion\"] = \"\"\n",
        "\n",
        "    # Filtrar registros\n",
        "    for i in range(df_registros_filtrados.shape[0]):\n",
        "        texto = str(df_registros_filtrados.loc[i, colTexto]).lower().replace(\"á\", \"a\").replace(\"é\", \"e\").replace(\"í\", \"i\").replace(\"ó\", \"o\").replace(\"ú\", \"u\")\n",
        "\n",
        "        # Buscar coincidencias con expresiones regulares\n",
        "        for expresion, razon in zip(expresiones_regex, df_terminos[\"categoría\"]):\n",
        "            coincidencias = expresion.findall(f\" {texto} \")\n",
        "            if coincidencias:\n",
        "                df_registros_filtrados.loc[i, \"contiene_termino\"] = True\n",
        "                df_registros_filtrados.loc[i, \"razon_eliminacion\"] = f\"Presencia de términos relacionados a {razon}\"\n",
        "                break\n",
        "            else:\n",
        "                df_registros_filtrados.loc[i, \"contiene_termino\"] = False\n",
        "                df_registros_filtrados.loc[i, \"razon_eliminacion\"] = f\"Ausencia de términos relacionados a {razon}\"\n",
        "\n",
        "    # Revisar uso elegido del diccionario, para descarte (negativo) o filtrado (positivo)\n",
        "    if cargar_diccionario == \"Diccionario de descarte (negativo)\":\n",
        "        # Mantener registros que no contengan términos del diccionario (negativo)\n",
        "        df_registros_filtrados_final = df_registros_filtrados[~df_registros_filtrados[\"contiene_termino\"]]\n",
        "        df_registros_eliminados = df_registros_filtrados[df_registros_filtrados[\"contiene_termino\"]]\n",
        "    elif cargar_diccionario == \"Diccionario de filtrado (positivo)\":\n",
        "        # Mantener registros que sí contengan al menos un término del diccionario (positivo)\n",
        "        df_registros_filtrados_final = df_registros_filtrados[df_registros_filtrados[\"contiene_termino\"]]\n",
        "        df_registros_eliminados = df_registros_filtrados[~df_registros_filtrados[\"contiene_termino\"]]\n",
        "\n",
        "    # Eliminar columnas auxiliares para tabla con registros depurados\n",
        "    df_registros_filtrados_final = df_registros_filtrados_final.drop(columns=[\"contiene_termino\", \"razon_eliminacion\"])\n",
        "\n",
        "    df_registros_filtrados_final = df_registros_filtrados_final.reset_index(drop=True)\n",
        "    df_registros_eliminados = df_registros_eliminados.reset_index(drop=True)\n",
        "\n",
        "    return df_registros_filtrados_final, df_registros_eliminados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCZ7eKNn9Raj"
      },
      "outputs": [],
      "source": [
        "# Definir el nombre de la columna que contiene el texto, si no está ya definido\n",
        "# Asegúrate de que esta columna exista en el DataFrame dfClean\n",
        "if 'text_column' not in globals():\n",
        "    text_column = 'sem_text'  # Cambia 'sem_text' por el nombre correcto de la columna\n",
        "\n",
        "# Ejecutar depuración de registros por diccionario cargado\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "    try:\n",
        "        # Asegúrate de que dfDiccionario esté definido y cargado antes de la función\n",
        "        if 'dfDiccionario' in globals() and isinstance(dfDiccionario, pd.DataFrame):\n",
        "            df_depurados_dicc, df_eliminados_dicc = filtrar_registros(dfClean, dfDiccionario, text_column, cargar_diccionario.value)\n",
        "        else:\n",
        "            print(\"El diccionario no está cargado o no es un DataFrame válido.\")\n",
        "    except NameError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "else:\n",
        "    print(\"No se cargó ningún diccionario...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBzKJUPi9SD9"
      },
      "source": [
        "**Previsualizar registros depurados y eliminados (opcional):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RccykSdQ9Wjc"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de registros eliminados y verificar su número de filas y columnas\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  try:\n",
        "    display(df_eliminados_dicc)\n",
        "    print(f\"\\nFilas/Columnas (shape) en registros eliminados por diccionario: {df_eliminados_dicc.shape}\")\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBBkL_qZ9YE1"
      },
      "outputs": [],
      "source": [
        "# Previsualizar data frame final\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "    dfFinal = df_depurados_dicc\n",
        "    dfFinal.to_csv(f\"{project_name.value}_filtrados-dicc.csv\")\n",
        "    print(f\"{project_name.value}_filtrados-dicc.csv\")\n",
        "else:\n",
        "    dfFinal = dfClean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HauJQB3v8xTF"
      },
      "source": [
        "## **4. Análisis exploratorio de datos (visualizaciones de metadatos de anuncios y su segmentación de audiencias)**\n",
        "*Se recomienda utilizar esta sección con conjuntos de datos no mayores a 20,000 filas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqir9kJjg8F_"
      },
      "source": [
        "### Resumen estadistico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWssq7OHhO0A"
      },
      "outputs": [],
      "source": [
        "# Seleccionar columnas numericas\n",
        "columnas_numericas = ['spend_min', 'spend_max', 'impressions_min', 'impressions_max', 'audience_min', 'audience_max','spend_avg', 'audience_avg', 'impressions_avg', 'campaign_duration']\n",
        "# columnas_numericas = ['spend_min', 'spend_max', 'impressions_min', 'impressions_max','spend_avg', 'impressions_avg', 'campaign_duration']\n",
        "datos_numericos = dfFinal[columnas_numericas]\n",
        "\n",
        "# Crear resumen\n",
        "resumen_estadistico = datos_numericos.describe()\n",
        "\n",
        "# Mostrar resultados\n",
        "resumen_estadistico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcLZFCjCkEB4"
      },
      "source": [
        "### Distribución temporal de los anuncios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBuHOccajRvL"
      },
      "outputs": [],
      "source": [
        "def plot_temporal_distribution(df, column, bins=30, kde=True, color='skyblue'):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(df[column], bins=bins, kde=kde, color=color)\n",
        "    plt.title(f'Distribución Temporal de {column}')\n",
        "    plt.xlabel('Fecha de Creación del Anuncio')\n",
        "    plt.ylabel('Frecuencia')\n",
        "    plt.show()\n",
        "\n",
        "# Mostrar resultados\n",
        "plot_temporal_distribution(dfFinal, 'ad_creation_time')\n",
        "plot_temporal_distribution(dfFinal, 'campaign_duration')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM0agFwYy6iu"
      },
      "source": [
        "### Análisis de audiencia segmentada, gasto e impresiones a través del tiempo:\n",
        "(Puede tomar algo de tiempo...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwtaF10oBR4n"
      },
      "outputs": [],
      "source": [
        "def plot_lineplot(df, x_column, y_column, hue_column, title, x_label, y_label):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.lineplot(x=x_column, y=y_column, data=df, hue=hue_column)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.legend(title=hue_column)\n",
        "    plt.show()\n",
        "\n",
        "# Mostrar resultados\n",
        "plot_lineplot(dfFinal, 'ad_creation_time', 'audience_avg', 'page_name', 'Avg Audience Size a lo largo del Tiempo', 'Fecha de Creación del Anuncio', 'Avg Audience Size')\n",
        "plot_lineplot(dfFinal, 'ad_creation_time', 'spend_avg', 'page_name', 'Avg Spend a lo largo del Tiempo', 'Fecha de Creación del Anuncio', 'Avg Spend')\n",
        "plot_lineplot(dfFinal, 'ad_creation_time', 'impressions_avg', 'page_name', 'Avg Impressions a lo largo del Tiempo', 'Fecha de Creación del Anuncio', 'Avg Impressions')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKXKJbYNDZUU"
      },
      "source": [
        "### Costo promedio por impresion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twzZssmkDO2v"
      },
      "outputs": [],
      "source": [
        "# Calcular el CPI & CPM\n",
        "cpi_promedio = dfFinal['spend_avg'].mean() / dfFinal['impressions_avg'].mean()\n",
        "cpm_promedio = cpi_promedio * 1000\n",
        "\n",
        "# Mostrar resultados\n",
        "print('Costo por Impresion',cpi_promedio,'...', 'Costo por 1000 Impresiones', cpm_promedio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INESI1YxF0Gn"
      },
      "source": [
        "### Matriz de Correlación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5vLuhhz7WX6"
      },
      "outputs": [],
      "source": [
        "# Seleccionar columnas numericas\n",
        "columnas_numericas = ['spend_min', 'impressions_min', 'audience_min', 'campaign_duration',\n",
        "                      'spend_max', 'impressions_max', 'audience_max', 'campaign_duration',\n",
        "                      'spend_avg', 'audience_avg', 'impressions_avg', 'campaign_duration']\n",
        "\n",
        "# Calcular la matriz de correlación\n",
        "correlation_matrix = dfFinal[columnas_numericas].corr()\n",
        "\n",
        "# Mostrar resultados\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title(\"Matriz de Correlación entre Variables Numéricas\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amx81N8RGPqi"
      },
      "source": [
        "### Analisis de Dispersion y Relacion entre Variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW1BO0aD41EV"
      },
      "outputs": [],
      "source": [
        "def scatter_matrix(dataframe, numeric_columns):\n",
        "    sns.pairplot(dataframe[numeric_columns])\n",
        "    plt.suptitle(\"Relaciones entre Variables Numéricas\", y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "# Variables numericas de interes\n",
        "# columnas_numericas_min = ['spend_min', 'impressions_min', 'campaign_duration']\n",
        "# columnas_numericas_max = ['spend_max', 'impressions_max', 'campaign_duration']\n",
        "# columnas_numericas_avg = ['spend_avg', 'impressions_avg', 'campaign_duration']\n",
        "\n",
        "\n",
        "columnas_numericas_min = ['spend_min', 'impressions_min', 'audience_min', 'campaign_duration']\n",
        "columnas_numericas_max = ['spend_max', 'impressions_max', 'audience_max', 'campaign_duration']\n",
        "columnas_numericas_avg = ['spend_avg', 'audience_avg', 'impressions_avg', 'campaign_duration']\n",
        "\n",
        "# Mostrar resultados\n",
        "scatter_matrix(dfFinal, columnas_numericas_min)\n",
        "scatter_matrix(dfFinal, columnas_numericas_max)\n",
        "scatter_matrix(dfFinal, columnas_numericas_avg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqUOo9I7zjyl"
      },
      "source": [
        "### Visualización de variables categóricas de anuncios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GyBOX8_lDO8"
      },
      "outputs": [],
      "source": [
        "def plot_categorical_distribution(df, column, top_n=10, figsize=(12, 6)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    top_categories = df[column].value_counts().nlargest(top_n)\n",
        "\n",
        "    sns.barplot(x=top_categories.values, y=top_categories.index, hue=top_categories.index, palette='viridis', legend=False)\n",
        "\n",
        "    plt.title(f'Distribución de {column}')\n",
        "    plt.xlabel('Frecuencia')\n",
        "    plt.ylabel(column)\n",
        "    plt.show()\n",
        "\n",
        "# Seleccionar columnas categoricas\n",
        "columnas_categoricas = ['page_name', 'byline', 'ad_creative_link_titles', 'ad_creative_bodies',\n",
        "                        'ad_creative_link_captions', 'ad_creative_link_descriptions',\n",
        "                        'publisher_platforms', 'languages']\n",
        "\n",
        "# Mostrar resultados\n",
        "for column in columnas_categoricas:\n",
        "    plot_categorical_distribution(dfFinal, column)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMVLKFJT1rf3"
      },
      "source": [
        "### Distribución normalizada de segmentación de audiencias de anuncios por Edad y Género:\n",
        "(Puede tomar algo de tiempo...)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZnTY7FiCba8"
      },
      "outputs": [],
      "source": [
        "def create_normalized_df(df, column_name, id_column='ad_archive_id', subcolumn_names=None):\n",
        "    df_normalized = pd.DataFrame()\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        for entry in row[column_name]:\n",
        "            subcolumn_values = [entry[subcolumn] for subcolumn in subcolumn_names] if subcolumn_names else []\n",
        "            percentage = entry['percentage']\n",
        "\n",
        "            data = {id_column: [row[id_column]], **dict(zip(subcolumn_names, subcolumn_values)), 'percentage': [percentage]}\n",
        "            df_normalized = pd.concat([df_normalized, pd.DataFrame(data)])\n",
        "\n",
        "    total_percentage_by_group = df_normalized.groupby(subcolumn_names)['percentage'].sum()\n",
        "    total_percentage_normalized = total_percentage_by_group / total_percentage_by_group.sum()\n",
        "\n",
        "    df_normalized_final = pd.DataFrame({\n",
        "        **{subcolumn: total_percentage_normalized.index.get_level_values(subcolumn) for subcolumn in subcolumn_names},\n",
        "        'percentage': total_percentage_normalized.values * 100 # indicar porcentaje sobre 100%\n",
        "    })\n",
        "\n",
        "    return df_normalized_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNhAPZSfi7Q6"
      },
      "outputs": [],
      "source": [
        "# Crear dataframe con los datos expandidos\n",
        "df_demographics_normalized = create_normalized_df(dfFinal, 'demographic_distribution_dict', subcolumn_names=['age', 'gender'])\n",
        "\n",
        "# Crear tabla y mostrar\n",
        "fig_normalized = px.treemap(df_demographics_normalized, path=['age','gender'], values='percentage',\n",
        "                            title='Treemap de Distribución Demográfica Normalizada',\n",
        "                            color='percentage',\n",
        "                            color_continuous_scale='viridis')\n",
        "\n",
        "fig_normalized.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKO5uIWRxSZp"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de porcentajes de distribución por región\n",
        "df_sorted_demographics =df_demographics_normalized.sort_values(by='percentage', ascending=False)\n",
        "\n",
        "df_sorted_demographics.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2UTNkySzsyM"
      },
      "outputs": [],
      "source": [
        "# Descargar archivo de datos CSV con valores normalizados de distribución por región:\n",
        "df_sorted_demographics.to_csv(f\"{project_name.value}_demographics.csv\", index=False)\n",
        "\n",
        "print(f\"{project_name.value}_demographics.csv descargado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydcoZsSj1z0h"
      },
      "source": [
        "### Distribución geográfica de anuncios (por Región):\n",
        "(Puede tomar algo de tiempo...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJFdkqJ_vnx0"
      },
      "outputs": [],
      "source": [
        "def create_normalized_regions_df(df, column_name, id_column='ad_archive_id', subcolumn_names=None):\n",
        "    df_normalized = pd.DataFrame()\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        for entry in row[column_name]:\n",
        "            subcolumn_values = [entry[subcolumn] for subcolumn in subcolumn_names] if subcolumn_names else []\n",
        "            percentage = entry['percentage']\n",
        "\n",
        "            data = {id_column: [row[id_column]], **dict(zip(subcolumn_names, subcolumn_values)), 'percentage': [percentage]}\n",
        "            df_normalized = pd.concat([df_normalized, pd.DataFrame(data)])\n",
        "\n",
        "    total_percentage_by_group = df_normalized.groupby(subcolumn_names)['percentage'].sum()\n",
        "    total_percentage_normalized = total_percentage_by_group / total_percentage_by_group.sum()\n",
        "\n",
        "    df_normalized_regions_final = pd.DataFrame({\n",
        "        **{subcolumn: total_percentage_normalized.index.get_level_values(subcolumn) for subcolumn in subcolumn_names},\n",
        "        'percentage': total_percentage_normalized.values*100 # indicar porcentaje sobre 100%\n",
        "    })\n",
        "\n",
        "    return df_normalized_regions_final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KawhPDWit5l_"
      },
      "outputs": [],
      "source": [
        "# Crear dataframe con los datos expandidos por región\n",
        "df_regions_normalized = create_normalized_regions_df(dfFinal, 'delivery_by_region_dict', subcolumn_names=['region'])\n",
        "\n",
        "# Crear tabla y mostrar\n",
        "fig_normalized_regions = px.treemap(df_regions_normalized, path=['region'], values='percentage',\n",
        "                            title='Treemap de Distribución por Región',\n",
        "                            color='percentage',\n",
        "                            color_continuous_scale='viridis')\n",
        "\n",
        "fig_normalized_regions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNQV5g5kxFyu"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de porcentajes de distribución por región\n",
        "df_sorted_regions = df_regions_normalized.sort_values(by='percentage', ascending=False)\n",
        "df_sorted_regions.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQyS5_4FzEkf"
      },
      "outputs": [],
      "source": [
        "# Descargar archivo de datos CSV con valores normalizados de distribución por región:\n",
        "df_sorted_regions.to_csv(f\"{project_name.value}_regions.csv\", index=False)\n",
        "\n",
        "print(f\"{project_name.value}_regions.csv descargado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTgWNBUPmsQn"
      },
      "source": [
        "### Desplegar mapa de regiones por país (trabajo en progreso, solo EEUU, México y España, por ahora):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnRNvF5WjAoQ"
      },
      "outputs": [],
      "source": [
        "# Función para transformar nombres de regiones\n",
        "def transform_region_name(region):\n",
        "    # Quitar acentos y caracteres especiales\n",
        "    no_accents = unidecode.unidecode(region)\n",
        "    # Convertir a mayúsculas\n",
        "    upper_case = no_accents.upper()\n",
        "    return upper_case\n",
        "\n",
        "# Crear nuevo dataframe\n",
        "df_transformed = df_sorted_regions.copy()\n",
        "\n",
        "# Apicar transformación a nueva columna\n",
        "df_transformed['region_transformed'] = df_transformed['region'].apply(transform_region_name)\n",
        "\n",
        "# Desplegar nuevo dataframe\n",
        "df_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mjc4gYRdptn"
      },
      "outputs": [],
      "source": [
        "# US estados\n",
        "url = 'https://raw.githubusercontent.com/signalab/postprocesador-redes/refs/heads/main/anexos/mapas/us-states.json'\n",
        "\n",
        "# MX estados\n",
        "# url = 'https://raw.githubusercontent.com/signalab/postprocesador-redes/refs/heads/main/anexos/mapas/mx-states.json'\n",
        "\n",
        "# ES comunidades autónomas\n",
        "# url = 'https://raw.githubusercontent.com/signalab/postprocesador-redes/refs/heads/main/anexos/mapas/es-ccaa.json'\n",
        "\n",
        "\n",
        "\n",
        "# Fetch datos GeoJSON\n",
        "response = requests.get(url)\n",
        "geojson_data = response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwQ2t9lTe-m6"
      },
      "outputs": [],
      "source": [
        "# # Crear mapa base\n",
        "m = folium.Map(location=[0, 0], zoom_start=3)\n",
        "\n",
        "# # Añadir capa de mapa coroplético\n",
        "folium.Choropleth(\n",
        "    geo_data=geojson_data,\n",
        "    name='choropleth',\n",
        "    data=df_transformed,\n",
        "    columns=['region', 'percentage'],\n",
        "    key_on='feature.properties.name',\n",
        "    fill_color='Blues',\n",
        "    fill_opacity=0.7,\n",
        "    line_opacity=0.2,\n",
        "    legend_name='Porcentaje de anuncios por Región',\n",
        "    nan_fill_color='gray',  # Color for regions with no data\n",
        "    nan_fill_opacity=0.7,\n",
        "    bins=5,  # Número de binas para escala de color\n",
        "    reset=True  # Reincializar mapa antes de añadir capa coroplética\n",
        ").add_to(m)\n",
        "\n",
        "# # Añadir control sobre capa\n",
        "folium.LayerControl().add_to(m)\n",
        "\n",
        "# # Desplega mapa\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFAVQTVDk66d"
      },
      "source": [
        "## **5. Referencias**\n",
        "\n",
        "*  Bird, Steven, Edward Loper & Ewan Klein (2009).\n",
        "Natural Language Processing with Python.  O'Reilly Media Inc.\n",
        "* Kiss, T., & Strunk, J. (2006). Unsupervised Multilingual Sentence Boundary Detection. Computational Linguistics, 32(4), 485-525. https://doi.org/10.1162/coli.2006.32.4.485]\n",
        "* Meta. (2024) Meta AdLibrary [software]. https://www.facebook.com/ads/library/\n",
        "* Rieder, B. (2024) YouTube Data Tools [software]. Digital Methods Initiative. https://ytdt.digitalmethods.net/\n",
        "\n",
        "*Programación asistida ocasionalmente con herramientas de IA Generativa: ChatGPT, Phind, HuggingChat y Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwTqBwqGhnLp"
      },
      "source": [
        "## **6. Créditos**\n",
        "\n",
        "**Realizado por el equipo de Signa_Lab ITESO:**\n",
        "\n",
        "- **Programación de cuadernos de código (Python)**:\n",
        " José Luis Almendarez González y Diego Arredondo Ortiz.\n",
        "\n",
        "- **Supervisión del desarrollo tecnológico y documentación:**\n",
        "Diego Arredondo Ortiz\n",
        "\n",
        "- **Equipo de Coordinación Signa_Lab ITESO:**\n",
        "Paloma López Portillo Vázquez, Víctor Hugo Ábrego Molina y Eduardo G. de Quevedo Sánchez\n",
        "\n",
        "Noviembre, 2024. Instituto Tecnológico y de Estudios Superiores de Occidente (ITESO)\n",
        "Tlaquepaque, Jalisco, México.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "krmVOSpgxeyN",
        "xlViHlO_x5lv",
        "1VtBL2ysyAkF",
        "pfE26SQw6oF3",
        "pYzRA7yY7Ote",
        "rXl-EjuU7XMr",
        "uyVU2PR91HVP",
        "QFGvqSeK8r3c",
        "ZOfAWch29EUR",
        "Qqir9kJjg8F_",
        "QcLZFCjCkEB4",
        "NM0agFwYy6iu",
        "NKXKJbYNDZUU",
        "INESI1YxF0Gn",
        "amx81N8RGPqi",
        "eqUOo9I7zjyl",
        "pMVLKFJT1rf3",
        "ydcoZsSj1z0h",
        "YTgWNBUPmsQn"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

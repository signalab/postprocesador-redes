{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HtQx5zDeIb0"
      },
      "source": [
        "# **Signa_Lab ITESO:** Postprocesador de datos de redes sociodigitales\n",
        "## **Cuaderno: YouTube (listas de videos o canales)**\n",
        "\n",
        "Cuaderno de código abierto diseñado para importar datos sobre listas de videos o canales desde la API de YouTube, como las arrojadas por la herramienta YouTube Data Tools, y preparar sus datos para análisis exploratorio, con herramientas para la visualización de datos y técnicas de análisis semántico.\n",
        "\n",
        "**\\***Los grupos de celdas marcadas con **asterisco requieren información** antes de seguir adelante."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **0. Introducción**\n",
        "\n",
        "Antes de comenzar, necesitas obtener al menos un archivo en formato **CSV o Excel** que registre **listas de videos o canales de YouTube** y sus metadatos.\n",
        "\n",
        "Puedes leer la documentación de la YouTube Data API para saber más sobre los datos consultables, los operadores de búsqueda y cómo hacer implementaciones propias.\n",
        "\n",
        "Para descargar fácilmente datos de videos o canales, puedes usar la herramienta YouTube Data Tools, desarrollada por Bernhard Rieder desde Digital Methods Initiative.\n",
        "\n",
        "**Los siguientes módulos de [YouTube Data Tools](https://ytdt.digitalmethods.net/) son compatibles con este código:**\n",
        "\n",
        "\n",
        "*   YouTube **Video List** ([**descargar datos**](https://ytdt.digitalmethods.net/mod_videos_list.php) / [documentación de API](https://developers.google.com/youtube/v3/docs/videos))\n",
        "\n",
        "*   YouTube **Channel List** ([**descargar datos**](https://ytdt.digitalmethods.net/mod_channels_list.php) / [documentación de API](https://developers.google.com/youtube/v3/docs/channels))\n",
        "\n",
        "\n",
        "Puedes consultar en este [video un tutorial](https://ytdt.digitalmethods.net/)\n",
        "sobre cómo usar los distintos módulos de Youtube Data Tools, publicado por su creador, Bernhard Rieder.\n"
      ],
      "metadata": {
        "id": "D9xY77YEEZ2z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD-wOGZQ5Ki6"
      },
      "source": [
        "---\n",
        "\n",
        "## **1. Importar librerías y archivos de datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalar e importar librerías:"
      ],
      "metadata": {
        "id": "ReN-KpvrJ-jS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOuliduEPJtw"
      },
      "source": [
        "**Instalar** librerías."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kidkEvPHPJtw"
      },
      "outputs": [],
      "source": [
        "# Instalar librerías de Python necesarias\n",
        "\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install difflib\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install scipy\n",
        "!pip install numpy\n",
        "!pip install plotly\n",
        "!pip install time\n",
        "!pip install tqdm\n",
        "!pip install operator\n",
        "!pip install wordcloud\n",
        "!pip install networkx\n",
        "# !pip install scikit-learn\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi3kGn_FLC6T"
      },
      "source": [
        "**Importar librerías** necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ3xsq9LK2Sr"
      },
      "outputs": [],
      "source": [
        "# Importar librerías de Python necesarias\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import sys\n",
        "from difflib import SequenceMatcher\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from scipy.stats import gaussian_kde\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import math\n",
        "import operator\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder, TrigramCollocationFinder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Indicar rutas de archivos de datos a importar y nombre de proyecto:"
      ],
      "metadata": {
        "id": "o2-KsJ87KOiZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfsq2zLpPJtx"
      },
      "source": [
        "**Importar y concatenar archivos de datos** (en CSV o Excel):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para cargar archivos a partir de la extensión en su ruta indicada\n",
        "def load_file(path):\n",
        "    if path.endswith('.csv'):\n",
        "        return pd.read_csv(path)\n",
        "    elif path.endswith('.xlsx'):\n",
        "        return pd.read_excel(path)\n",
        "    else:\n",
        "        raise ValueError(\"Formato no compatible. Por favor carga solo archivos .csv or .xlsx.\")\n",
        "\n",
        "# Inicializar lista para alojar todas las rutas y una variable para el DataFrame final, accesible globalmente\n",
        "file_paths = []\n",
        "dfs = []\n",
        "df = None  # DataFrame global\n",
        "\n",
        "# Definir función para añadir un nuevo campo de texto (input) para añadir una ruta de archivo adicional\n",
        "def add_file_input(b=None):\n",
        "    path_input = widgets.Text(value='', placeholder='Escribe la ruta del archivo', description=f'Archivo {len(file_paths) + 1}:')\n",
        "    file_paths.append(path_input)\n",
        "    update_ui()\n",
        "\n",
        "# Definir función para eliminar el último campo de texto (input) para ruta de archivo\n",
        "def remove_file_input(b=None):\n",
        "    if file_paths:\n",
        "        file_paths.pop()\n",
        "        update_ui()\n",
        "\n",
        "# Definir función para procesar y cargar todos los archivos\n",
        "def process_files(b):\n",
        "    global dfs, df\n",
        "    dfs = []  # Vaciar DataFrames\n",
        "\n",
        "    for path_input in file_paths:\n",
        "        path = path_input.value\n",
        "        try:\n",
        "            temp_df = load_file(path)\n",
        "            temp_df['filename'] = path  # Add a column with the filename\n",
        "            dfs.append(temp_df)\n",
        "            print(f\"Nombre de archivo: {path}\")\n",
        "            print(f\"Filas/Columnas (shape): {temp_df.shape}\")\n",
        "        except ValueError as e:\n",
        "            print(f\"Error al cargar el archivo {path}: {e}\")\n",
        "            return\n",
        "\n",
        "    if dfs:\n",
        "        df = pd.concat(dfs, ignore_index=True)  # Concatenate all DataFrames\n",
        "        print(\"\\n¡Se cargaron todos los archivos!\")\n",
        "        print(f\"Filas/Columnas (shape) de DataFrame creado: {df.shape}\")\n",
        "\n",
        "# Campo de texto (input) para indicar nombre del proyecto (para integrarse en nombres de archivos a exportar)\n",
        "project_name = widgets.Text(value='', placeholder='Escribe el nombre del proyecto (corto y sin espacios)', description='Nombre proyecto:')\n",
        "\n",
        "# Botones para añadir y eliminar archivos\n",
        "add_button = widgets.Button(description=\"Añadir archivo\",  button_style='')\n",
        "remove_button = widgets.Button(description=\"Eliminar archivo\",  button_style='warning')\n",
        "load_button = widgets.Button(description=\"Cargar archivos\",  button_style='primary')\n",
        "\n",
        "add_button.on_click(add_file_input)\n",
        "remove_button.on_click(remove_file_input)\n",
        "load_button.on_click(process_files)\n",
        "\n",
        "# Definir función para actualizar UI\n",
        "def update_ui():\n",
        "    clear_output()\n",
        "    display(project_name)\n",
        "    for path_input in file_paths:\n",
        "        display(path_input)\n",
        "    display(widgets.HBox([add_button, remove_button]))\n",
        "    display(load_button)\n",
        "\n",
        "# Inicializar UI con un campo de texto (input) para ruta de archivo\n",
        "add_file_input()\n"
      ],
      "metadata": {
        "id": "hEtma1lgB6Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previsualizar datos importados:"
      ],
      "metadata": {
        "id": "O9kQer1lRAg5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HptxpF1lHby"
      },
      "source": [
        "**Previsualizar tabla** con todos los registros importados:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsualizar dataframe con CSVs importados\n",
        "display(df)\n",
        "print(f\"Filas/Columnas (shape) en registros importados: {df.shape}\")\n"
      ],
      "metadata": {
        "id": "IM2Rv28GEHOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exportar copia en CSV con registros importados (o concatenados):**"
      ],
      "metadata": {
        "id": "9xaQzFYoasLs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nheXTnM7PJt0"
      },
      "outputs": [],
      "source": [
        "# Exportar archivo CSV con tabla de registros importados (y concatenados, en el caso de múltiples archivos)\n",
        "df.to_csv(f\"{project_name.value}_registros-importados.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hPbcqVvlHbz"
      },
      "source": [
        "---\n",
        "\n",
        "## **2. Limpieza de registros importados**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generar identificadores únicos (IDs) por registro:"
      ],
      "metadata": {
        "id": "l8Fx8oCP7tYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para asignar IDs únicos a cada fila en el data frame indicado como parámetro, comenzando desde '1000001'.\n",
        "def assign_unique_ids(df):\n",
        "    # Inicializar contador para IDs\n",
        "    id_counter = 1000001\n",
        "\n",
        "    # Crear copia de data frame original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Verificar si el data frame tiene una columna llamada 'videoId'\n",
        "    if 'videoId' in df.columns:\n",
        "        # Si la columna 'videoId' existe, no hacer nada\n",
        "        pass\n",
        "    else:\n",
        "        # Si la columna 'videoId' no existe, duplicar la columna 'id' a una nueva columna llamada 'channelId'\n",
        "        df_copy['channelId'] = df_copy['id'].copy()\n",
        "\n",
        "    # Iterar a través de las filas del data frame\n",
        "    for index, _ in enumerate(df_copy.index):\n",
        "        # Dar formato a ID con ceros adicionales e incorporarlo al data frame\n",
        "        formatted_id = str(id_counter).zfill(7)  # Se asegura de que sea un ID de 7 dígitos, agregando ceros cuando sea necesario\n",
        "        df_copy.loc[index, 'id'] = formatted_id\n",
        "\n",
        "        # Incrementar el contador del ID para la siguiente iteración\n",
        "        id_counter += 1\n",
        "\n",
        "    return df_copy"
      ],
      "metadata": {
        "id": "EQHpLDhpSCC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar función para asignar IDs a cada registro y previsualizar tabla\n",
        "if __name__ == \"__main__\":\n",
        "    # Invocar la función con data frame de trabajo\n",
        "    df_ids = assign_unique_ids(df)\n",
        "\n",
        "# Sobreescribir data frame con nueva tabla con IDs generados\n",
        "df = df_ids\n",
        "df"
      ],
      "metadata": {
        "id": "UyPVvBIWsVzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVfoXAtilHbz"
      },
      "source": [
        "### *Elegir columna y criterios para limpieza de texto sin aporte semántico:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para limpiar usuarios, hashtags y URLs\n",
        "def limpiar_texto(texto, eliminar_usuarios, eliminar_hashtags, eliminar_urls, regex_personalizado):\n",
        "    # Eliminar usuarios si está activado\n",
        "    if eliminar_usuarios:\n",
        "        texto = re.sub(r\"(?<!\\w)@(\\w+)(?!\\w)\", \"\", texto)\n",
        "\n",
        "    # Eliminar hashtags si está activado\n",
        "    if eliminar_hashtags:\n",
        "        texto = re.sub(r\"(?<!\\w)#(\\w+)(?!\\w)\", \"\", texto)\n",
        "\n",
        "    # Eliminar URLs si está activado\n",
        "    if eliminar_urls:\n",
        "        texto = re.sub(r\"(http|https|ftp)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\", \"\", texto)\n",
        "        texto = texto.lstrip(\". \")\n",
        "\n",
        "    # Aplicar regex personalizado si se proporciona\n",
        "    if regex_personalizado:\n",
        "        texto = re.sub(regex_personalizado, \"\", texto)\n",
        "\n",
        "    return texto.strip()\n",
        "\n",
        "# Definir función para agregar nueva columna con texto limpio (clean_text)\n",
        "def agregarCleanTextADf(df, colText, eliminar_usuarios, eliminar_hashtags, eliminar_urls, regex_personalizado):\n",
        "    dfW = df.copy()\n",
        "    dfW[\"clean_text\"] = None\n",
        "\n",
        "    for index, row in dfW.iterrows():\n",
        "        text = str(row[colText])\n",
        "        cleaned_text = limpiar_texto(text, eliminar_usuarios, eliminar_hashtags, eliminar_urls, regex_personalizado)\n",
        "        dfW.at[index, \"clean_text\"] = cleaned_text\n",
        "\n",
        "    return dfW\n",
        "\n",
        "\n",
        "# Crear widget de desplegable con columnas disponibles\n",
        "available_columns = list(df.columns)\n",
        "column_dropdown = widgets.Dropdown(options=available_columns, value=available_columns[0], description='Columna:')\n",
        "\n",
        "\n",
        "# Definir widgets de IPyWidgets para la UI interactiva\n",
        "eliminar_usuarios = widgets.Checkbox(value=True, description='Eliminar usuarios @')\n",
        "eliminar_hashtags = widgets.Checkbox(value=True, description='Eliminar hashtags #')\n",
        "eliminar_urls = widgets.Checkbox(value=True, description='Eliminar URLs')\n",
        "regex_input = widgets.Text(value='', description='Otro (Regex):', placeholder='Escribe un regex opcional')\n",
        "\n",
        "\n",
        "# Botón para ejecutar la limpieza\n",
        "boton_limpiar = widgets.Button(description=\"Limpiar texto\", button_style='warning')\n",
        "\n",
        "# Caja de texto para mostrar el resultado\n",
        "output_resultado = widgets.Output()\n",
        "\n",
        "# Función que se ejecuta al hacer clic en el botón de limpiar texto\n",
        "def ejecutar_limpieza(b):\n",
        "    global dfCleanText  # Hacer que dfCleanText sea accesible globalmente\n",
        "    with output_resultado:\n",
        "        output_resultado.clear_output()  # Limpiar cualquier salida previa\n",
        "\n",
        "        # Obtener el nombre de la columna seleccionada por el usuario\n",
        "        text_column = column_dropdown.value\n",
        "\n",
        "        # Verificar si la columna existe en el DataFrame\n",
        "        if text_column not in df.columns:\n",
        "            print(f\"Error: La columna '{text_column}' no existe en el DataFrame.\")\n",
        "            return\n",
        "\n",
        "        # Ejecutar la función de limpieza sobre el DataFrame seleccionado\n",
        "        dfCleanText = agregarCleanTextADf(\n",
        "            df, text_column,\n",
        "            eliminar_usuarios.value,\n",
        "            eliminar_hashtags.value,\n",
        "            eliminar_urls.value,\n",
        "            regex_input.value\n",
        "        )\n",
        "\n",
        "        # Mostrar el DataFrame con la nueva columna 'clean_text'\n",
        "        print(\"Texto limpio aplicado. DataFrame actualizado globalmente como 'dfCleanText'.\")\n",
        "        display(dfCleanText)\n",
        "\n",
        "# Conectar el botón con la función de limpieza\n",
        "boton_limpiar.on_click(ejecutar_limpieza)\n",
        "\n",
        "# Desplegar los widgets en pantalla\n",
        "display(column_dropdown, eliminar_usuarios, eliminar_hashtags, eliminar_urls, regex_input, boton_limpiar, output_resultado)\n"
      ],
      "metadata": {
        "id": "9LINWys5czcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f86YTwWJlHb0"
      },
      "source": [
        "### \\*Elegir idioma de palabras vacías (*stop words*) a eliminar:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMi4M_w-lHb0"
      },
      "source": [
        "Elegir **idioma de** (desde diccionarios de NLTK), **eliminar *stop words*** y **agregar columna sem_text** con texto depurado:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Definir función para eliminar stopwords y signos de puntuación\n",
        "def delete_stopwords(texto, stopwords_list):\n",
        "    # Tokenizar el texto\n",
        "    tokens = nltk.word_tokenize(texto)\n",
        "\n",
        "    # Eliminar signos de puntuación\n",
        "    tokens = [token for token in tokens if token.isalnum()]\n",
        "\n",
        "    # Eliminar stop words\n",
        "    tokens = [token for token in tokens if token.lower() not in stopwords_list]\n",
        "\n",
        "    # Convertir la lista de tokens a un string\n",
        "    texto_limpio = \" \".join(tokens)\n",
        "\n",
        "    return texto_limpio.strip()\n",
        "\n",
        "# Definir función para eliminar palabras vacías y agregar nueva columna con el resultado (sem_text)\n",
        "def agregarSemTextADf(df, colText, stopwords_list):\n",
        "    dfW = df.copy()\n",
        "    dfW[\"sem_text\"] = None\n",
        "\n",
        "    for index, row in dfW.iterrows():\n",
        "        text = row[colText]\n",
        "        cleaned_text = delete_stopwords(text, stopwords_list)\n",
        "        dfW.at[index, \"sem_text\"] = cleaned_text\n",
        "\n",
        "    return dfW\n",
        "\n",
        "# Lista de idiomas disponibles en NLTK\n",
        "idiomas_stopwords = [\n",
        "    'arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french',\n",
        "    'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali',\n",
        "    'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish',\n",
        "    'swedish', 'tajik', 'turkish'\n",
        "]\n",
        "\n",
        "# Crear dropdown para seleccionar idioma\n",
        "dropdown_idioma = widgets.Dropdown(\n",
        "    options=idiomas_stopwords,\n",
        "    value='spanish',  # Valor por defecto\n",
        "    description='Idioma:'\n",
        ")\n",
        "\n",
        "# Botón para eliminar stopwords\n",
        "boton_eliminar_stopwords = widgets.Button(description=\"Eliminar stopwords\", button_style='warning')\n",
        "\n",
        "# Output para mostrar el resultado\n",
        "output_resultado_stopwords = widgets.Output()\n",
        "\n",
        "# Función que se ejecuta al hacer clic en el botón de eliminar stopwords\n",
        "def ejecutar_eliminacion_stopwords(b):\n",
        "    global dfClean  # Hacer que dfClean sea accesible globalmente\n",
        "    with output_resultado_stopwords:\n",
        "        output_resultado_stopwords.clear_output()  # Limpiar cualquier salida previa\n",
        "\n",
        "        # Obtener el idioma seleccionado\n",
        "        idioma_seleccionado = dropdown_idioma.value\n",
        "\n",
        "        # Descargar stopwords del idioma seleccionado\n",
        "        stopwords_list = stopwords.words(idioma_seleccionado)\n",
        "\n",
        "        # Verificar si la columna 'clean_text' existe en el DataFrame\n",
        "        if 'clean_text' not in dfCleanText.columns:\n",
        "            print(\"Error: La columna 'clean_text' no existe en el DataFrame.\")\n",
        "            return\n",
        "\n",
        "        # Ejecutar la función para eliminar stopwords y agregar la nueva columna 'sem_text'\n",
        "        dfClean = agregarSemTextADf(dfCleanText, 'clean_text', stopwords_list)\n",
        "\n",
        "        # Mostrar el DataFrame con la nueva columna 'sem_text'\n",
        "        print(f\"Stopwords en {idioma_seleccionado} eliminadas. DataFrame actualizado globalmente como 'dfClean'.\")\n",
        "        display(dfClean)\n",
        "\n",
        "# Conectar el botón con la función de eliminación de stopwords\n",
        "boton_eliminar_stopwords.on_click(ejecutar_eliminacion_stopwords)\n",
        "\n",
        "# Desplegar los widgets en pantalla\n",
        "display(dropdown_idioma, boton_eliminar_stopwords, output_resultado_stopwords)\n",
        "\n",
        "# Exportar archivo CSV con tabla completa de registros importados con IDs y texto sin palabras vacías (sem_text)\n",
        "# dfClean.to_csv(f\"{project_name.value}_registros-semtext.csv\")\n"
      ],
      "metadata": {
        "id": "kQtNOs0Ulz2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Procesar duraciones, ratio de alcance y/o concatenación de URLs"
      ],
      "metadata": {
        "id": "L9hV0p5Y-0d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para procesar listas de videos o canales\n",
        "\n",
        "def listParse(df):\n",
        "    # Comprobamos si el dataframe tiene una columna 'videoId'\n",
        "    if 'videoId' in df.columns:\n",
        "        # Si es una lista de videos, creamos las columnas adicionales\n",
        "        # durationMin: duración en minutos con 2 decimales\n",
        "        df['durationMin'] = df['durationSec'] / 60.0\n",
        "        df['durationMin'] = df['durationMin'].round(2)\n",
        "\n",
        "        # durationRuntime: duración en formato hh:mm:ss\n",
        "        df['durationRuntime'] = df['durationSec'].apply(lambda x: '{:02d}:{:02d}:{:02d}'.format(int(x // 3600), int((x % 3600) // 60), int(x % 60)))\n",
        "\n",
        "        # videoURL: URL completa del video\n",
        "        df['videoURL'] = 'https://www.youtube.com/watch?v=' + df['videoId']\n",
        "\n",
        "        # channelURL: URL completa del canal\n",
        "        df['channelURL'] = 'https://www.youtube.com/channel/' + df['channelId']\n",
        "\n",
        "        # Eliminamos registros duplicados según videoId\n",
        "        df = df.drop_duplicates(subset='videoId', keep='first')\n",
        "    else:\n",
        "        # Si es una lista de canales, creamos las columnas adicionales\n",
        "        # channelURL: URL completa del canal\n",
        "        # df['channelURL'] = 'https://www.youtube.com/channel/' + df['id']\n",
        "\n",
        "        # reachRatio: relación de alcance (viewCount / videoCount) * 100\n",
        "        df['reachRatio'] = (df['videoCount'] / df['viewCount']) * 100\n",
        "        df['reachRatio'] = df['reachRatio'].round(4)\n",
        "\n",
        "        # Eliminamos registros duplicados según channelId\n",
        "        df = df.drop_duplicates(subset='id', keep='first')\n",
        "\n",
        "    return df\n",
        "\n",
        "listParse(dfClean)"
      ],
      "metadata": {
        "id": "bprMHTi4Ofns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHNNKJdTlHb1"
      },
      "source": [
        "---\n",
        "\n",
        "## **3. Depuración de registros desde diccionarios personalizados (con términos de filtrado o descarte)**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Definir uso de de diccionario de depuración:"
      ],
      "metadata": {
        "id": "KL71R1c4O6cp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t-7ye11YwN2"
      },
      "source": [
        "A partir de diccionarios personalizados con términos de descarte o filtración, elige depurar o mantener, respectivamente, registros que los contengan.\n",
        "\n",
        "El diccionario debe cargarse en formato CSV y contener, al menos, los siguientes campos:\n",
        "\n",
        "| palabra | tipo | categoría | diccionario |\n",
        "|---------|------|-----------|-------------|\n",
        "|         |      |           |             |\n",
        "|         |      |           |             |\n",
        "|         |      |           |             |\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "| palabra | tipo   | categoría   | diccionario        |\n",
        "|---------|--------|-------------|--------------------|\n",
        "| idiota  | ofensa | humillación | ofensa-humillación |\n",
        "| zorra   | ofensa | género      | ofensa-género      |\n",
        "|         |        |             |                    |\n",
        "\n",
        "Puedes [encontrar aquí una copia del diccionario](https://drive.google.com/file/d/1zK214W0pBRYn9lnY_MDJYhEEc6pI3L6F/view?usp=sharing) con la estructura requerida, en formato CSV, para descargar, llenar e incorporar a este cuaderno de código.\n",
        "\n",
        "**Nota:** La versión actual de este cuaderno de código permite elegir entre las siguientes opciones sobre la carga de diccionarios:\n",
        "\n",
        "\n",
        "- ***No cargar diccionario:*** Ignorar esta funcionalidad y no usar diccionarios.\n",
        "- ***Diccionario de descarte (negativo):*** Se eliminarán todos los registros que mencionen alguno de esos términos.\n",
        "- ***Diccionario de filtrado (positivo):*** Se mantendrán solo los registros que mencionen alguno de esos términos."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dropdown widget for dictionary usage selection\n",
        "cargar_diccionario = widgets.Dropdown(\n",
        "    options=[\"No cargar diccionario\", \"Diccionario de descarte (negativo)\", \"Diccionario de filtrado (positivo)\"],\n",
        "    value=\"No cargar diccionario\",\n",
        "    description=\"Elegir uso:\"\n",
        ")\n",
        "\n",
        "# Create the text input widget for file path\n",
        "rutaDicc = widgets.Text(\n",
        "    value=\"\",\n",
        "    placeholder=\"Escribir ruta a archivo CSV\",\n",
        "    description=\"Ruta:\"\n",
        ")\n",
        "\n",
        "# Create the \"Aceptar\" button\n",
        "accept_button = widgets.Button(description=\"Aceptar\")\n",
        "\n",
        "# Create an output widget to display results or errors\n",
        "output = widgets.Output()\n",
        "\n",
        "# Function to load the dictionary based on user input\n",
        "def load_dictionary(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "            try:\n",
        "                global dfDiccionario\n",
        "                dfDiccionario = pd.read_csv(rutaDicc.value)\n",
        "                print(f\"Archivo de diccionario '{rutaDicc.value}' cargado exitosamente.\")\n",
        "                display(dfDiccionario.head())  # Show the first few rows of the loaded dictionary\n",
        "            except FileNotFoundError:\n",
        "                print(f\"El archivo de diccionario '{rutaDicc.value}' no fue encontrado. Por favor, verifique la ruta.\")\n",
        "        else:\n",
        "            dfDiccionario = None\n",
        "            print(\"No se cargó ningún diccionario.\")\n",
        "\n",
        "# Link the button to the load function\n",
        "accept_button.on_click(load_dictionary)\n",
        "\n",
        "# Display the widgets\n",
        "display(cargar_diccionario, rutaDicc, accept_button, output)\n"
      ],
      "metadata": {
        "id": "AuzxsTVCg-uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar uso de diccionario elegido\n",
        "print(f\"Uso de diccionario elegido: {cargar_diccionario.value}\")"
      ],
      "metadata": {
        "id": "24zZKqwCSgo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aplicar filtrado por diccionario y previsualizar resultados (opcional, solo si se cargó algún diccionario):"
      ],
      "metadata": {
        "id": "MS2u9S1B-W2_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hj8iG14lHb2"
      },
      "source": [
        "**Previsualizar diccionario de descarte importado:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_D1HZSAlHb2"
      },
      "outputs": [],
      "source": [
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  # Previsualizar tabla de diccionario cargado (en caso de haber elegido utilizar diccionarios)\n",
        "  try:\n",
        "      display(dfDiccionario.head())  # Previsualizar primeros registros de diccionario\n",
        "      print(f\"\\nFilas y columnas en diccionario cargado:\")  # Verificar número de filas y columnas en diccionario\n",
        "      print(f\"Shape: {dfDiccionario.shape}\")  # Verificar número de filas y columnas en diccionario\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filtrar registros a partir de diccionario cargado y modalidad de depuración (positiva o negativa):**"
      ],
      "metadata": {
        "id": "JVUqxmIe6LHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filtrar_registros(df_registros, df_terminos, colTexto, cargar_diccionario):\n",
        "    # Definir los términos del diccionario\n",
        "    terminos = df_terminos[\"palabra\"].tolist()\n",
        "    # Compilar expresiones regulares una sola vez\n",
        "    expresiones_regex = [\n",
        "        re.compile(r\"(?<!\\S)?(?:\\s|[.,;:?!¡¿]){}(?:\\s|[.,;:?!¡¿])?(?!\\S)\".format(re.escape(termino)), re.IGNORECASE)\n",
        "        for termino in terminos\n",
        "    ]\n",
        "\n",
        "    # Copiar el dataframe de registros y agregar columnas auxiliares\n",
        "    df_registros_filtrados = df_registros.copy()\n",
        "    df_registros_filtrados[\"contiene_termino\"] = False\n",
        "    df_registros_filtrados[\"razon_eliminacion\"] = \"\"\n",
        "\n",
        "    # Filtrar registros\n",
        "    for i in range(df_registros_filtrados.shape[0]):\n",
        "        texto = str(df_registros_filtrados.loc[i, colTexto]).lower().replace(\"á\", \"a\").replace(\"é\", \"e\").replace(\"í\", \"i\").replace(\"ó\", \"o\").replace(\"ú\", \"u\")\n",
        "\n",
        "        # Buscar coincidencias con expresiones regulares\n",
        "        for expresion, razon in zip(expresiones_regex, df_terminos[\"categoría\"]):\n",
        "            coincidencias = expresion.findall(f\" {texto} \")\n",
        "            if coincidencias:\n",
        "                df_registros_filtrados.loc[i, \"contiene_termino\"] = True\n",
        "                df_registros_filtrados.loc[i, \"razon_eliminacion\"] = f\"Presencia de términos relacionados a {razon}\"\n",
        "                break\n",
        "            else:\n",
        "                df_registros_filtrados.loc[i, \"contiene_termino\"] = False\n",
        "                df_registros_filtrados.loc[i, \"razon_eliminacion\"] = f\"Ausencia de términos relacionados a {razon}\"\n",
        "\n",
        "    # Revisar uso elegido del diccionario, para descarte (negativo) o filtrado (positivo)\n",
        "    if cargar_diccionario == \"Diccionario de descarte (negativo)\":\n",
        "        # Mantener registros que no contengan términos del diccionario (negativo)\n",
        "        df_registros_filtrados_final = df_registros_filtrados[~df_registros_filtrados[\"contiene_termino\"]]\n",
        "        df_registros_eliminados = df_registros_filtrados[df_registros_filtrados[\"contiene_termino\"]]\n",
        "    elif cargar_diccionario == \"Diccionario de filtrado (positivo)\":\n",
        "        # Mantener registros que sí contengan al menos un término del diccionario (positivo)\n",
        "        df_registros_filtrados_final = df_registros_filtrados[df_registros_filtrados[\"contiene_termino\"]]\n",
        "        df_registros_eliminados = df_registros_filtrados[~df_registros_filtrados[\"contiene_termino\"]]\n",
        "\n",
        "    # Eliminar columnas auxiliares para tabla con registros depurados\n",
        "    df_registros_filtrados_final = df_registros_filtrados_final.drop(columns=[\"contiene_termino\", \"razon_eliminacion\"])\n",
        "\n",
        "    df_registros_filtrados_final = df_registros_filtrados_final.reset_index(drop=True)\n",
        "    df_registros_eliminados = df_registros_eliminados.reset_index(drop=True)\n",
        "\n",
        "    return df_registros_filtrados_final, df_registros_eliminados\n"
      ],
      "metadata": {
        "id": "R6lyxcaMHx6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dgm6-sYlHb2"
      },
      "outputs": [],
      "source": [
        "# Definir el nombre de la columna que contiene el texto, si no está ya definido\n",
        "# Asegúrate de que esta columna exista en el DataFrame dfClean\n",
        "if 'text_column' not in globals():\n",
        "    text_column = 'sem_text'  # Cambia 'sem_text' por el nombre correcto de la columna\n",
        "\n",
        "# Ejecutar depuración de registros por diccionario cargado\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "    try:\n",
        "        # Asegúrate de que dfDiccionario esté definido y cargado antes de la función\n",
        "        if 'dfDiccionario' in globals() and isinstance(dfDiccionario, pd.DataFrame):\n",
        "            df_depurados_dicc, df_eliminados_dicc = filtrar_registros(dfClean, dfDiccionario, text_column, cargar_diccionario.value)\n",
        "        else:\n",
        "            print(\"El diccionario no está cargado o no es un DataFrame válido.\")\n",
        "    except NameError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "else:\n",
        "    print(\"No se cargó ningún diccionario...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Previsualizar registros depurados y eliminados (opcional):**"
      ],
      "metadata": {
        "id": "U4dnWce36Uek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsualizar tabla de registros depurados (conservados) y verificar número de filas y columna\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  try:\n",
        "    display(df_depurados_dicc)\n",
        "    print(f\"\\nFilas/Columnas (shape) en registros conservados por diccionario: {df_depurados_dicc.shape}\")\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")"
      ],
      "metadata": {
        "id": "RGDYqSyOlZVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsualizar tabla de registros eliminados y verificar su número de filas y columnas\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  try:\n",
        "    display(df_eliminados_dicc)\n",
        "    print(f\"\\nFilas/Columnas (shape) en registros eliminados por diccionario: {df_eliminados_dicc.shape}\")\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")"
      ],
      "metadata": {
        "id": "8qHoZTZV6bZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsualizar data frame final\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "    dfFinal = df_depurados_dicc\n",
        "else:\n",
        "    dfFinal = dfClean\n",
        "\n",
        "display(dfFinal)"
      ],
      "metadata": {
        "id": "Z7QA0-A7SKMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **4. Análisis exploratorio de datos (visualizaciones de metadatos de videos o canales)**\n",
        "*Se recomienda utilizar esta sección con conjuntos de datos no mayores a 20,000 filas."
      ],
      "metadata": {
        "id": "94TJeKlg_GlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gráfico de burbujas (por vistas y fecha de publicación/creación):"
      ],
      "metadata": {
        "id": "6_4amKXBWK3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficar Bubble Chart por vistas y fecha de creación\n",
        "df = dfFinal  # dataframe con los datos\n",
        "df['viewCount'] = df['viewCount'].fillna(0)  # Fill NaN values with 0\n",
        "\n",
        "# Verificamos si el dataframe tiene una columna 'videoId'\n",
        "if 'videoId' in df.columns:\n",
        "    # Si es una lista de videos, configuramos las codificaciones para el eje x e y\n",
        "    x = 'publishedAt'\n",
        "    y = 'viewCount'\n",
        "    color = 'channelTitle'  # usaremos diferentes colores para cada título de canal\n",
        "    hover_name = 'videoTitle'  # mostraremos el título del video en la información emergente\n",
        "    title = f'YouTube: {df.shape[0]} videos por vistas y fecha de publicación'\n",
        "else:\n",
        "    # Si es una lista de canales, configuramos las codificaciones para el eje x e y\n",
        "    x = 'publishedAt'\n",
        "    y = 'viewCount'\n",
        "    color = 'title'  # usaremos diferentes colores para cada título de canal\n",
        "    hover_name = 'title'  # mostraremos el título del canal en la información emergente\n",
        "    title = f'YouTube: {df.shape[0]} canales por vistas acumuladas y fecha de creación \\n {project_name.value}'\n",
        "\n",
        "# Convertimos la columna 'publishedAt' a objetos de fecha de Python\n",
        "df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n",
        "\n",
        "# Creamos el gráfico de burbujas con Plotly Express\n",
        "fig = px.scatter(df, x=x, y=y, color=color, hover_name=hover_name, title=title,\n",
        "                 size='viewCount',  # codificamos el tamaño de cada punto según la cantidad de vistas\n",
        "                 opacity=0.6)  # configuramos la opacidad de cada punto al 60%\n",
        "\n",
        "\n",
        "# Configuramos el eje x para que muestre fechas\n",
        "fig.update_xaxes(title='Fecha de publicación', type='date')\n",
        "\n",
        "# Configuramos el eje y para que muestre la cantidad de vistas\n",
        "fig.update_yaxes(title='Cantidad de vistas')\n",
        "\n",
        "fig.add_annotation(\n",
        "    text=f'Datos: {project_name.value}',\n",
        "    xref='paper',\n",
        "    yref='paper',\n",
        "    x=0,\n",
        "    y=-0.2,\n",
        "    showarrow=False,\n",
        "    font_size=12,\n",
        "    font_color='gray'\n",
        ")\n",
        "\n",
        "# Mostramos el gráfico\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "k3Y5Y8-ITu_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mapa de árbol (por canal y vistas):\n",
        "\n",
        "*En el caso de listas de videos, la visualización muestra por default solo videos con más de 1000 vistas. Este umbral (`threshold`) puede ajustarse en el código y volverse a ejecutar."
      ],
      "metadata": {
        "id": "IKbbyS4XWcs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficar mapa de árbol (treemap)\n",
        "df = dfFinal  # dataframe con los datos\n",
        "threshold = 1000 # para listas de videos, filtro por número de vistas\n",
        "\n",
        "# Verificamos si el dataframe tiene una columna 'videoId'\n",
        "if 'videoId' in df.columns:\n",
        "    # Si es una lista de videos, configuramos las codificaciones para el treemap\n",
        "    df_filtered = df[df['viewCount'] >= threshold]\n",
        "    path = ['channelTitle', 'videoTitle']  # jerarquía para agrupar videos por canal\n",
        "    values = 'viewCount'\n",
        "    color = 'channelTitle'  # usaremos diferentes colores para cada título de canal\n",
        "    title = f'YouTube: {df_filtered.shape[0]} videos por canal y título con más de {threshold} vistas'\n",
        "else:\n",
        "    # Si es una lista de canales, configuramos las codificaciones para el treemap\n",
        "    df_filtered = df\n",
        "    path = ['title']  # solo mostramos el título del canal\n",
        "    values = 'viewCount'\n",
        "    color = 'title'  # usaremos diferentes colores para cada título de canal\n",
        "    title = f'YouTube: {df.shape[0]} canales por vistas acumuladas'\n",
        "\n",
        "# Creamos el gráfico de treemap con Plotly Express\n",
        "fig = px.treemap(df_filtered, path=path, values=values, color=color, title=title)\n",
        "\n",
        "# Configuramos el título del gráfico\n",
        "fig.update_layout(title=dict(y=0.95, x=0.5, xanchor='center'))\n",
        "\n",
        "# Add-a-custom-annotation\n",
        "fig.add_annotation(\n",
        "    text=f'Datos: {project_name.value}',\n",
        "    xref='paper',\n",
        "    yref='paper',\n",
        "    x=0,\n",
        "    y=-0.2,\n",
        "    showarrow=False,\n",
        "    font_size=12,\n",
        "    font_color='gray'\n",
        ")\n",
        "\n",
        "# Configurar despliegue de tamaño de texto\n",
        "fig.update_traces(texttemplate='%{label}&lt;br&gt;%{value}', textinfo='label+value', textfont=dict(size=12))\n",
        "\n",
        "# Mostramos el gráfico\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "-Kktblq7b7o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gráfico de líneas múltiples (por vistas, canal y fecha de publicación de videos):\n",
        "*Solo compatible con listas de videos"
      ],
      "metadata": {
        "id": "InAh-Z8wWrVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gráfica de líneas múltiples. Alcance en vistas a videos por canal\n",
        "df = dfFinal  # dataframe con los datos\n",
        "\n",
        "# Verificamos si el dataframe tiene una columna 'videoId'\n",
        "if 'videoId' in df.columns:\n",
        "    # Si es una lista de videos, configuramos las codificaciones para el eje x e y\n",
        "    x = 'publishedAt'\n",
        "    y = 'viewCount'\n",
        "    color = 'channelTitle'  # usaremos diferentes colores para cada ID de canal\n",
        "    hover_name = 'videoTitle'  # mostraremos el título del video en la información emergente\n",
        "    title = f'YouTube: {df.shape[0]} videos por vistas y fecha de publicación'\n",
        "\n",
        "    # Ordenamos el dataframe por fecha de publicación y ID de canal\n",
        "    df = df.sort_values(by=[x, 'channelId'])\n",
        "\n",
        "else:\n",
        "    # Si es una lista de canales, configuramos las codificaciones para el eje x e y\n",
        "    x = 'publishedAt'\n",
        "    y = 'viewCount'\n",
        "    color = 'title'  # usaremos diferentes colores para cada ID de canal\n",
        "    hover_name = 'title'  # mostraremos el título del canal en la información emergente\n",
        "    title = f'YouTube: {df.shape[0]} canales por vistas acumuladas y fecha de creación \\n {project_name.value}'\n",
        "\n",
        "# Convertimos la columna 'publishedAt' a objetos de fecha de Python\n",
        "df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n",
        "\n",
        "# Creamos el gráfico de líneas múltiples con Plotly Express\n",
        "fig = px.line(df, x=x, y=y, color=color, hover_name=hover_name, title=title)\n",
        "\n",
        "# Configuramos la opacidad de las líneas\n",
        "fig.update_traces(opacity=0.6)\n",
        "\n",
        "# Configuramos el eje x para que muestre fechas\n",
        "fig.update_xaxes(title='Fecha de publicación', type='date', fixedrange=True)\n",
        "\n",
        "# Configuramos el eje y para que muestre la cantidad de vistas\n",
        "fig.update_yaxes(title='Cantidad de vistas', fixedrange=True)\n",
        "\n",
        "# Configuramos el color para que utilice un máximo de 64 colores\n",
        "# fig.update_layout(coloraxis_colorbar_nticks=64)\n",
        "\n",
        "fig.add_annotation(\n",
        "    text=f'Datos: {project_name.value}',\n",
        "    xref='paper',\n",
        "    yref='paper',\n",
        "    x=0,\n",
        "    y=-0.2,\n",
        "    showarrow=False,\n",
        "    font_size=12,\n",
        "    font_color='gray'\n",
        ")\n",
        "\n",
        "# Mostramos el gráfico\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "DLoeVMbddwf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nube de palabras (por títulos de videos o canales, según el tipo de lista cargada):"
      ],
      "metadata": {
        "id": "K6t8K6AYW3wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficar Nube de Palabras\n",
        "df = dfFinal  # dataframe con los datos\n",
        "\n",
        "# Verificamos si el dataframe tiene una columna 'videoId'\n",
        "if 'videoId' in df.columns:\n",
        "    # Si es una lista de videos, utilizamos la columna 'videoTitle'\n",
        "    text =''.join(df['sem_text'].tolist())\n",
        "else:\n",
        "    # Si es una lista de canales, utilizamos la columna 'channelTitle'\n",
        "    text =''.join(df['sem_text'].tolist())\n",
        "\n",
        "# Verificamos si el dataframe tiene una columna 'videoId'\n",
        "wordcloudTitle = ''\n",
        "if 'videoId' in df.columns:\n",
        "  wordcloudTitle = f'YouTube: Palabras más repetidas en {df.shape[0]} nombres de videos'\n",
        "\n",
        "else:\n",
        "  wordcloudTitle = f'YouTube: Palabras más repetidas en {df.shape[0]} nombres de canales'\n",
        "\n",
        "\n",
        "\n",
        "# Creamos el wordcloud\n",
        "wordcloud = WordCloud(width=960, height=960, max_font_size=110).generate(text)\n",
        "\n",
        "# Convertimos el wordcloud a una imagen\n",
        "img = wordcloud.to_image()\n",
        "\n",
        "# Creamos un gráfico interactivo con plotly\n",
        "fig = go.Figure(data=[go.Image(z=img)])\n",
        "fig.update_layout(title=wordcloudTitle, width=960, height=960)\n",
        "\n",
        "# Mostramos el gráfico\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "a7UnWizjhqNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigrama (grafo de relaciones entre pares de palabras):"
      ],
      "metadata": {
        "id": "I98sFfTxYvzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definir función para identificar relaciones entre pares de palabras (bigramas) más frecuentes:"
      ],
      "metadata": {
        "id": "0u4QXnjgq1IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para calcular bigramas indicando un clúster específico u omitirlo para aplicarse con la muestra completa\n",
        "def calculate_bigrams(df, nCluster=None):\n",
        "    # Inicializar una lista para alojar todas las palabras en columna 'pregunta'\n",
        "    all_words = []\n",
        "\n",
        "    if nCluster is not None:\n",
        "      dfW = df[df[\"cluster\"] == nCluster]\n",
        "      df = pd.DataFrame(dfW[\"sem_text\"])\n",
        "    else:\n",
        "      df = pd.DataFrame(df[\"sem_text\"])\n",
        "\n",
        "    # Correr filtro por stopwords\n",
        "    stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "    for pregunta in df['sem_text']:\n",
        "        # Tokenizar textos de columna pregunta text por palabra\n",
        "        tokens = word_tokenize(pregunta, language='spanish')\n",
        "        # Filtrar stopwords de tokens\n",
        "        filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
        "        all_words.extend(filtered_words)\n",
        "\n",
        "    # Configurar identificador de bigramas\n",
        "    bigram_measures = BigramAssocMeasures()\n",
        "    bigram_finder = BigramCollocationFinder.from_words(all_words)\n",
        "\n",
        "    # Calcular peso de bigramas utilizando frecuencia\n",
        "    bigrams = bigram_finder.score_ngrams(bigram_measures.raw_freq)\n",
        "\n",
        "    # Convertir bigramas calculados y su frecuencia en dataFrame\n",
        "    bigrams_df = pd.DataFrame([(src, tgt, weight) for ((src, tgt), weight) in bigrams],\n",
        "                              columns=['source', 'target', 'weight'])\n",
        "\n",
        "    bigrams_df = bigrams_df.sort_values(by='weight', ascending=False)  # Ordenar bigramas por frecuencia\n",
        "\n",
        "    return bigrams_df  # Regresar dataFrame con lista bigramas ordenados"
      ],
      "metadata": {
        "id": "fUdjdcRQq2f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutar función para identificar relaciones entre pares de palabras (bigramas) más frecuentes en toda la muestra:\n",
        "\n"
      ],
      "metadata": {
        "id": "gxry6pdZq-eR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar cálculo de bigramas\n",
        "bigrama_muestra = calculate_bigrams(df)"
      ],
      "metadata": {
        "id": "YXDZGZCOrCUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigrama_muestra"
      ],
      "metadata": {
        "id": "cGTHgaOnrD_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_bigrams_muestra = bigrama_muestra.head(20)\n",
        "\n",
        "# Crear un gráfico de barras horizontal\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(top_bigrams_muestra['source'] + ' ' + top_bigrams_muestra['target'], top_bigrams_muestra['weight'], color='skyblue')\n",
        "plt.xlabel('Frecuencia')\n",
        "plt.ylabel('Bigrama')\n",
        "plt.title(f'Top 20 Bigramas en Muestra Completa del Tema {project_name.value}')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Q75pcYarHeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_bigrams(bigrams_df, top_n=100):\n",
        "    # Crear un grafo dirigido\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Añadir nodos y bordes al grafo con pesos\n",
        "    for _, row in bigrams_df.iterrows():\n",
        "        G.add_edge(row['source'], row['target'], weight=row['weight'])\n",
        "\n",
        "    # Calcular el grado ponderado de cada nodo\n",
        "    weighted_degree = dict(G.degree(weight='weight'))\n",
        "\n",
        "    # Ordenar nodos por grado ponderado y seleccionar los top_n\n",
        "    top_nodes = sorted(weighted_degree.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "    top_nodes = set(node for node, _ in top_nodes)\n",
        "\n",
        "    # Crear un subgrafo con los nodos seleccionados\n",
        "    H = G.subgraph(top_nodes).copy()\n",
        "\n",
        "    # Obtener posiciones de los nodos usando el layout de Fruchterman-Reingold\n",
        "    pos = nx.spring_layout(H, seed=42)\n",
        "\n",
        "    # Obtener los bordes y los pesos para visualización\n",
        "    edge_trace = go.Scatter(\n",
        "        x=[],\n",
        "        y=[],\n",
        "        line=dict(width=0.5, color='#888'),\n",
        "        hoverinfo='none',\n",
        "        mode='lines'\n",
        "    )\n",
        "\n",
        "    for edge in H.edges(data=True):\n",
        "        x0, y0 = pos[edge[0]]\n",
        "        x1, y1 = pos[edge[1]]\n",
        "        edge_trace['x'] += (x0, x1, None)\n",
        "        edge_trace['y'] += (y0, y1, None)\n",
        "\n",
        "    # Obtener los nodos para visualización\n",
        "    node_trace = go.Scatter(\n",
        "        x=[],\n",
        "        y=[],\n",
        "        text=[],\n",
        "        mode='markers+text',\n",
        "        textposition='top center',\n",
        "        marker=dict(\n",
        "            size=[],  # Dejar espacio para ajustar tamaño de nodos\n",
        "            color='#1f78b4',\n",
        "            line=dict(width=2, color='rgb(0,0,0)')\n",
        "        )\n",
        "    )\n",
        "\n",
        "    for node in H.nodes():\n",
        "        x, y = pos[node]\n",
        "        node_trace['x'] += (x,)\n",
        "        node_trace['y'] += (y,)\n",
        "        node_trace['text'] += (node,)\n",
        "        # Escalar el tamaño de los nodos por el grado ponderado\n",
        "        node_trace['marker']['size'] += (weighted_degree[node] * 1000,)\n",
        "\n",
        "    # Crear la visualización interactiva con Plotly\n",
        "    fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                    layout=go.Layout(\n",
        "                        showlegend=False,\n",
        "                        hovermode='closest',\n",
        "                        margin=dict(b=0, l=0, r=0, t=0)\n",
        "                    ))\n",
        "\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "ZTz5bbIwrL90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar grafo con bigramas de top 100 nodos por grado con pesos\n",
        "bigrams_df = calculate_bigrams(df)  # Calcular bigramas\n",
        "visualize_bigrams(bigrams_df)  # Visualizar bigramas"
      ],
      "metadata": {
        "id": "48H1HlNZrPNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **5. Consolidar y exportar archivos de datos procesados (registros conservados y elminados por diccionario)**"
      ],
      "metadata": {
        "id": "qV-AkoZwXNFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exportar archivos CSV con tablas finales de registros depurados y eliminados:"
      ],
      "metadata": {
        "id": "Qi6zLeREQCLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  try:\n",
        "    # Ejecutar concatenación de filas eliminadas por términos en diccionario\n",
        "    df_eliminados_final = df_eliminados_dicc\n",
        "    print(f\"Filas/Columnas (shape) en registros totales eliminados: {df_eliminados_final.shape}\")\n",
        "    display(df_eliminados_final)\n",
        "\n",
        "\n",
        "    # df_eliminados_final = pd.concat([df_eliminados_dicc, df_removed_duplicates], axis=0)\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")"
      ],
      "metadata": {
        "id": "NmmwiXBxUl0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsualizar tabla de datos con registros conservados\n",
        "dfFinal"
      ],
      "metadata": {
        "id": "36c-75ssU01e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuLWEw8llHb2"
      },
      "source": [
        "**Exportar archivo de datos (en formato CSV) con registros conservados y procesados:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkCxiCz-lHb2"
      },
      "outputs": [],
      "source": [
        "# Ejemplo exportar archivo de datos (CSV) con población de registros depurados\n",
        "dfFinal.to_csv(f\"{project_name.value}_registros-procesados.csv\")\n",
        "\n",
        "print(f\"¡{project_name.value}_registros-procesados.csv descargado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O5EaHqz8rXG"
      },
      "source": [
        "**Exportar archivo de datos (en formato CSV) de registros eliminados por términos de descarte, con su razonamiento correspondiente (en caso de haber cargado algún diccionario):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDjyzPwSE2qg"
      },
      "outputs": [],
      "source": [
        "# Ejemplo exportar archivo de datos (CSV) con registros eliminados por términos de descarte en diccionarios y repeticiones\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  try:\n",
        "    df_eliminados_final.to_csv(f\"{project_name.value}_registros-eliminados.csv\")\n",
        "    print(f\"¡{project_name.value}_registros-eliminados.csv descargado!\")\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3rKx63aGRWC"
      },
      "source": [
        "---\n",
        "\n",
        "## **6. Referencias**\n",
        "*  Bird, Steven, Edward Loper & Ewan Klein (2009).\n",
        "Natural Language Processing with Python.  O'Reilly Media Inc.\n",
        "* Kiss, T., & Strunk, J. (2006). Unsupervised Multilingual Sentence Boundary Detection. Computational Linguistics, 32(4), 485-525. https://doi.org/10.1162/coli.2006.32.4.485\n",
        "\n",
        "\n",
        "*Programación asistida ocasionalmente con herramientas de IA Generativa: ChatGPT, Phind, Google Gemini y Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **7. Créditos**\n",
        "**Realizado por el equipo de Signa_Lab ITESO:**\n",
        "\n",
        "- **Programación de cuadernos de código (Python)**:\n",
        "Diego Arredondo Ortiz, José Luis Almendarez González y Javier de la Torre Silva\n",
        "\n",
        "- **Supervisión del desarrollo tecnológico y documentación:**\n",
        "Diego Arredondo Ortiz\n",
        "\n",
        "- **Equipo de Coordinación Signa_Lab ITESO:**\n",
        "Paloma López Portillo Vázquez, Víctor Hugo Ábrego Molina y Eduardo G. de Quevedo Sánchez\n",
        "\n",
        "Noviembre, 2024. Instituto Tecnológico y de Estudios Superiores de Occidente (ITESO)\n",
        "Tlaquepaque, Jalisco, México.\n"
      ],
      "metadata": {
        "id": "ASBLPGoSNBqe"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ReN-KpvrJ-jS",
        "o2-KsJ87KOiZ",
        "O9kQer1lRAg5",
        "l8Fx8oCP7tYv",
        "QVfoXAtilHbz",
        "f86YTwWJlHb0",
        "L9hV0p5Y-0d3",
        "KL71R1c4O6cp",
        "MS2u9S1B-W2_",
        "6_4amKXBWK3n",
        "IKbbyS4XWcs5",
        "InAh-Z8wWrVc",
        "K6t8K6AYW3wb",
        "I98sFfTxYvzY",
        "Qi6zLeREQCLZ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}